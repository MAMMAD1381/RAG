{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1xGeaM-oFVeEliD38KtLRTtP4lPrQVkz-","authorship_tag":"ABX9TyMANn3Mx2i464cgp6/xLgHc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"badcfdf845ad4b6bbce972a4bac3781f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e958f77de12b4d8c8e42822157e55d40","IPY_MODEL_ef4b73016437491d8626339ab247622c","IPY_MODEL_21451cfe14bb47cb863bbe357a7c1b9b"],"layout":"IPY_MODEL_7377d48b4173405e82c3fb49483acf93"}},"e958f77de12b4d8c8e42822157e55d40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6228e9167f6d4360ac9580c1ad752207","placeholder":"​","style":"IPY_MODEL_9b78fcf4360f46ffb7393ec8105ca8e1","value":"Loading checkpoint shards: 100%"}},"ef4b73016437491d8626339ab247622c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc98bdf39bc41319d48ad023d66f5eb","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d73c3544de444e70a70536d12894f0cf","value":3}},"21451cfe14bb47cb863bbe357a7c1b9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01c4fd3aa15e4dcbaf52b43432effb9a","placeholder":"​","style":"IPY_MODEL_bacf15460f564c3186c09bbd06ad4020","value":" 3/3 [00:47&lt;00:00, 13.18s/it]"}},"7377d48b4173405e82c3fb49483acf93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6228e9167f6d4360ac9580c1ad752207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b78fcf4360f46ffb7393ec8105ca8e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fc98bdf39bc41319d48ad023d66f5eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d73c3544de444e70a70536d12894f0cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01c4fd3aa15e4dcbaf52b43432effb9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bacf15460f564c3186c09bbd06ad4020":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/Colab Notebooks/RAG'"],"metadata":{"id":"5wT2fvm20bmm","executionInfo":{"status":"ok","timestamp":1716992491970,"user_tz":-210,"elapsed":387,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","print(torch.cuda.is_available())\n","print(torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zd5YQ-IfoDFN","executionInfo":{"status":"ok","timestamp":1716990786821,"user_tz":-210,"elapsed":593,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"0da63487-8878-42c4-91a7-b2fb6f6fe3f0"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","Tesla T4\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# Path to your local model\n","model_path = f\"{project_path}/models/gemma-2b-it.gguf\"\n","\n","# Load the model and tokenizer (adjust as necessary for your specific library)\n","# tokenizer = AutoTokenizer.from_pretrained(model_path)\n","# model = AutoModelForCausalLM.from_pretrained(model_path)\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n","model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n","\n","# Move the model to GPU\n","model.to('cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"U-lAfNM_oQw7","executionInfo":{"status":"error","timestamp":1716991216695,"user_tz":-210,"elapsed":13,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"dbb485d3-b019-4359-d62d-498029538cbd"},"execution_count":40,"outputs":[{"output_type":"error","ename":"OSError","evalue":"Incorrect path_or_model_id: '/content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    400\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-ca59dec2812a>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf'. Please provide either the path to a local folder or the repo_id of a model on the Hub."]}]},{"cell_type":"code","source":["# !export HUGGINGFACE_TOKEN=\"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\""],"metadata":{"id":"ZcIUxAd0sfwA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from huggingface_hub import login\n","import torch\n","\n","# Log in using your Hugging Face access token\n","login(\"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\")\n","\n","# Path to the gated model repository\n","model_name_or_path = f\"{project_path}/models\"\n","# model_name_or_path = \"google/gemma-2b\"\n","\n","# Load the tokenizer and model from the Hugging Face Hub\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","\n","# Check if GPU is available\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Move the model to GPU\n","model.to(device)\n","\n","# Input text\n","input_text = \"C'era una volta\"\n","\n","# Tokenize input and move to GPU\n","input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","\n","# Generate text\n","with torch.no_grad():\n","    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n","\n","# Decode the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","print(generated_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["badcfdf845ad4b6bbce972a4bac3781f","e958f77de12b4d8c8e42822157e55d40","ef4b73016437491d8626339ab247622c","21451cfe14bb47cb863bbe357a7c1b9b","7377d48b4173405e82c3fb49483acf93","6228e9167f6d4360ac9580c1ad752207","9b78fcf4360f46ffb7393ec8105ca8e1","0fc98bdf39bc41319d48ad023d66f5eb","d73c3544de444e70a70536d12894f0cf","01c4fd3aa15e4dcbaf52b43432effb9a","bacf15460f564c3186c09bbd06ad4020"]},"id":"w1dXjxknq1iG","executionInfo":{"status":"ok","timestamp":1716992823453,"user_tz":-210,"elapsed":56158,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"f0af2b0b-6608-47e2-a7f7-689c4c2f9514"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"badcfdf845ad4b6bbce972a4bac3781f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["C'era una volta un ragazzo che amava la musica.\n","\n","Un giorno, mentre leggeva un libro, si imbatté in un articolo che diceva che la musica poteva essere usata per trattare i problemi di salute mentale.\n","\n","\n"]}]},{"cell_type":"code","source":["print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yg8BbWBBv4g_","executionInfo":{"status":"ok","timestamp":1716992846559,"user_tz":-210,"elapsed":7,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"ec6ffc5f-f3ad-423e-f64d-daea34ef922e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["local_model_path = f'{project_path}/models'\n","tokenizer.save_pretrained(local_model_path)\n","model.save_pretrained(local_model_path)"],"metadata":{"id":"DHWb-mZPub4o","executionInfo":{"status":"ok","timestamp":1716992620070,"user_tz":-210,"elapsed":124943,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Input text\n","input_text = \"say random things?\"\n","\n","# Tokenize input and move to GPU\n","input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","\n","# Generate text\n","with torch.no_grad():\n","    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n","\n","# Decode the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","print(generated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNXXzIShuCSI","executionInfo":{"status":"ok","timestamp":1716992885926,"user_tz":-210,"elapsed":3357,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"8e74d27d-f223-4c2d-8275-3236c6903523"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["say random things?\n","\n","I'm not sure if this is a good idea, but I'm going to try it anyway.\n","\n","I'm going to say random things.\n","\n","I'm going to say random things.\n","\n","I\n"]}]}]}