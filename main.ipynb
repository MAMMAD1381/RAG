{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1xGeaM-oFVeEliD38KtLRTtP4lPrQVkz-","authorship_tag":"ABX9TyN1X07fQDzrFk9+8QhHlaWT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"XyuFSQGWlMrD"}},{"cell_type":"code","source":["!pip install -q chromadb sentence-transformers pymupdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xxPdB5Lm6zpx","executionInfo":{"status":"ok","timestamp":1717797446681,"user_tz":-210,"elapsed":122492,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"06433a15-f50f-44ec-d9db-c964a7152e77"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/526.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/526.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install -q chromadb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8AWjbKsIDL4","executionInfo":{"status":"ok","timestamp":1717669405776,"user_tz":-210,"elapsed":36632,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"4bfe6c54-a4a6-42a2-a667-5850509509ba","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install -q langchain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bS6oQJeVSRTH","executionInfo":{"status":"ok","timestamp":1717609462111,"user_tz":-210,"elapsed":9225,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"43a7c35d-195a-4b0b-df15-076807ba03ee","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.71)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","import logging\n","from transformers import (\n","    GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration,\n","    BertTokenizer, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering,\n","    GPTNeoForCausalLM, pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoModelForCausalLM,\n","    BartTokenizer, BartForConditionalGeneration, PegasusTokenizer, PegasusForConditionalGeneration\n",")\n","import torch\n","from chromadb import Client, Settings\n","from sentence_transformers import SentenceTransformer\n","import gc\n","import fitz"],"metadata":{"id":"mJQSWTpCxjb2","executionInfo":{"status":"ok","timestamp":1717800004794,"user_tz":-210,"elapsed":394,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/Colab Notebooks/RAG'"],"metadata":{"id":"5wT2fvm20bmm","executionInfo":{"status":"ok","timestamp":1717797467140,"user_tz":-210,"elapsed":8,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# GPU"],"metadata":{"id":"-51Pl889lKDn"}},{"cell_type":"code","source":["\n","print(torch.cuda.is_available())\n","# print(torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zd5YQ-IfoDFN","executionInfo":{"status":"ok","timestamp":1717797467140,"user_tz":-210,"elapsed":7,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"f3542e11-9a67-4a48-aef9-367365652e06"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["# Hugging Face Login"],"metadata":{"id":"dPnOa3oLlRy2"}},{"cell_type":"code","source":["# Log in using your Hugging Face access token\n","access_token = \"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\"\n","login(access_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QvoS3g0665l","executionInfo":{"status":"ok","timestamp":1717797467140,"user_tz":-210,"elapsed":6,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"81ad358f-2576-4732-9ca9-9463969dadea"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# LLM\n","\n","\n","---\n","\n","###key functions of LLM class:\n","\n","\n","*   **load_llm_local**: tries to load an llm from google drvie\n","*   **load_llm_online**: loads the llm from hugging face\n","*   **select_device**: if gpu is available it will select it\n","*   **generate_text**: it can generate text based on given prompt"],"metadata":{"id":"_com4NubP5Jv"}},{"cell_type":"code","source":["class LLM:\n","    model_classes = {\n","        'gpt2': (GPT2Tokenizer, GPT2LMHeadModel, 'gpt2'),\n","        't5': (T5Tokenizer, T5ForConditionalGeneration, 't5-small'),\n","        'bert': (BertTokenizer, BertForQuestionAnswering, 'bert-large-uncased-whole-word-masking-finetuned-squad'),\n","        'distil-bert': (DistilBertTokenizer, DistilBertForQuestionAnswering, 'distilbert-base-cased-distilled-squad'),\n","        'gpt-neo': (GPT2Tokenizer, GPTNeoForCausalLM, 'EleutherAI/gpt-neo-1.3B'),\n","        'gemma': (AutoTokenizer, AutoModelForCausalLM, 'google/gemma-2b-it')\n","    }\n","    def __init__(self, llm_type: str, load_online=False, save_model=False):\n","        self.device = self.select_device()\n","        self.tokenizer, self.model = self.load_llm(llm_type, load_online, save_model)\n","        self.model.to(self.device)\n","        logging.basicConfig(level=logging.INFO)\n","        logging.info(f\"Model {llm_type} loaded and moved to {self.device}.\")\n","\n","    def load_llm(self, llm_type: str, load_online: bool, save_model: bool):\n","\n","        tokenizer_class, model_class, model_path = self.model_classes[llm_type]\n","\n","        if not load_online:\n","            model_path = f\"{project_path}/models/{model_path}\"\n","\n","        tokenizer = tokenizer_class.from_pretrained(model_path)\n","        model = model_class.from_pretrained(model_path)\n","\n","        if save_model:\n","            tokenizer.save_pretrained(f'{project_path}/models/{model_path}')\n","            model.save_pretrained(f'{project_path}/models/{model_path}')\n","\n","        return tokenizer, model\n","\n","\n","    @staticmethod\n","    def select_device() -> str:\n","        return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        raise NotImplementedError(\"The generate_text method should be implemented by the subclass.\")\n","\n","    def free_memory(self):\n","        del self.model\n","        del self.tokenizer\n","        gc.collect()\n","        torch.cuda.empty_cache()"],"metadata":{"id":"DRAdIqF3kqJo","executionInfo":{"status":"ok","timestamp":1717797467141,"user_tz":-210,"elapsed":5,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## GPT2"],"metadata":{"id":"mTRHbbc58yIz"}},{"cell_type":"code","source":["class GPT2(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gpt2', load_online, save_model)\n","        self.max_length = 1024\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        # Truncate the context if necessary to fit within the 1024 token limit\n","        max_context_length = self.max_length - len(\"Context: \\nQuestion: \\nAnswer:\")\n","        context = context[:max_context_length]\n","\n","        # Construct the prompt with the truncated context\n","        prompt = f\"Context: {context}\\nQuestion: {input_text}\\nAnswer:\"\n","\n","        # Encode the prompt and generate the response\n","        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n","        outputs = self.model.generate(\n","            inputs,\n","            max_new_tokens=80,\n","            # max_length=80,\n","            temperature=0.7,\n","            top_p=0.9,\n","            top_k=50,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            do_sample=True\n","        )\n","\n","        # Decode the output and extract the response\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = response.replace(prompt, '').strip()\n","        return response.split('\\n')[0]\n"],"metadata":{"id":"54UnFDdY9Hzl","executionInfo":{"status":"ok","timestamp":1717802620786,"user_tz":-210,"elapsed":396,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":["## t5"],"metadata":{"id":"bTjBZyNpEezi"}},{"cell_type":"code","source":["class T5(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('t5', load_online, save_model)\n","        self.max_length = 512\n","\n","    def generate_text(self, input_text: str, context: str = None, truncate_context=True) -> str:\n","        if context and truncate_context:\n","            max_context_length = self.max_length - len(\"question:  context: \")\n","            context = context[:max_context_length]\n","\n","        prompt = f\"question: {input_text} context: {context}\" if context else f\"question: {input_text}\"\n","        input_ids = self.tokenizer.encode(prompt, return_tensors='pt', max_length=self.max_length, truncation=True).to(self.device)\n","        outputs = self.model.generate(input_ids, max_length=50, num_beams=1, early_stopping=False)\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return response"],"metadata":{"id":"CS6-G5GMEeDe","executionInfo":{"status":"ok","timestamp":1717802693315,"user_tz":-210,"elapsed":370,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":["## BERT"],"metadata":{"id":"kZqUha0OG81c"}},{"cell_type":"code","source":["class BERT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('bert', load_online, save_model)\n","        self.max_length = 512\n","\n","    def generate_text(self, input_text: str, context: str = '', truncate_context=True) -> str:\n","        if context and truncate_context:\n","            max_context_length = self.max_length - len(input_text)\n","            context = context[:max_context_length]\n","\n","        self.model.to(self.device)\n","        nlp = pipeline('question-answering', model=self.model, tokenizer=self.tokenizer, device=0 if self.device == 'cuda' else -1)\n","        result = nlp(question=input_text, context=context)\n","        return result['answer']"],"metadata":{"id":"00dae2leG-LT","executionInfo":{"status":"ok","timestamp":1717802694431,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":["## Distil BERT"],"metadata":{"id":"YBkBRlkZJMPx"}},{"cell_type":"code","source":["class DistilBERT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('distil-bert', load_online, save_model)\n","        self.max_length = 512\n","\n","    def generate_text(self, input_text: str, context: str = '', truncate_context=True) -> str:\n","        if context and truncate_context:\n","            max_context_length = self.max_length - len(input_text)\n","            context = context[:max_context_length]\n","\n","        nlp = pipeline('question-answering', model=self.model, tokenizer=self.tokenizer, device=0 if self.device == 'cuda' else -1)\n","        result = nlp(question=input_text, context=context)\n","        return result['answer']"],"metadata":{"id":"muUrhJFlJOUU","executionInfo":{"status":"ok","timestamp":1717802695275,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["## GPT-Neo"],"metadata":{"id":"Cme1-CrhLlJ2"}},{"cell_type":"code","source":["class NeoGPT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gpt-neo', load_online, save_model)\n","        self.max_length = 1024\n","\n","    def generate_text(self, input_text: str, context: str = None, truncate_context=True) -> str:\n","        if context and truncate_context:\n","            max_context_length = self.max_length - len(\"answer this question:  based on this context: \")\n","            context = context[:max_context_length]\n","\n","        prompt = f\"answer this question: {input_text}\\nbased on this context: {context}\" if context else f\"question: {input_text}\"\n","        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n","        outputs = self.model.generate(\n","            inputs,\n","            max_length=100,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_beams=5,\n","            temperature=0.7,\n","            top_k=50,\n","            top_p=0.95,\n","            no_repeat_ngram_size=2,\n","            do_sample=True\n","        )\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return response"],"metadata":{"id":"OlHq6oDMLnNc","executionInfo":{"status":"ok","timestamp":1717802696154,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":["## Gemma"],"metadata":{"id":"yJmD43xgSJrm"}},{"cell_type":"code","source":["class Gemma(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gemma', load_online, save_model)\n","        self.max_length = 512\n","\n","    def generate_text(self, input_text: str, context: str = '', truncate_context=True) -> str:\n","        if context and truncate_context:\n","            max_context_length = self.max_length - len(\"Context:  Question:  Answer:\")\n","            context = context[:max_context_length]\n","\n","        prompt = f\"Context: {context}\\nQuestion: {input_text}\\nAnswer:\"\n","        inputs = self.tokenizer(prompt, return_tensors='pt', max_length=self.max_length, truncation=True)\n","        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n","\n","        outputs = self.model.generate(\n","            **inputs,\n","            max_length=80,\n","            temperature=0.7,\n","            top_p=0.9,\n","            top_k=50,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            do_sample=True\n","        )\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = response.replace(prompt, '').strip()\n","\n","        return response"],"metadata":{"id":"LCgib4NxRuvv","executionInfo":{"status":"ok","timestamp":1717802697822,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":["## test LLM class"],"metadata":{"id":"Myfojpj6wtID"}},{"cell_type":"code","source":["llm = BERT()\n","# llm = Gemma(load_online=True)"],"metadata":{"id":"sBG1ITO4-Dbk","executionInfo":{"status":"ok","timestamp":1717802838951,"user_tz":-210,"elapsed":101916,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","# question = \"say my name.\"\n","# context = 'my name is walter white.'\n","question = \"What is the capital of Iran?\"\n","context = 'tehran is the capital of iran.'\n","context = \"\"\"Well, Prince, so Genoa and Lucca are now just family estates of the Buonapartes. But I warn you, if you don’t tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist—I really believe he is Antichrist—I will have nothing more to do with you and you are no longer my friend, no longer my ‘faithful slave’, as you call yourself! But how do you do? I see I have frightened you—sit down and tell me all the news.\"\n","\n","It was in July, 1805, and the speaker was the well-known Anna Pavlovna Scherer, maid of honour and favourite of the Empress Marya Fedorovna. With these words she greeted Prince Vasili Kuragin, a man of high rank and importance, who was the first to arrive at her reception. Anna Pavlovna had had a cough for some days. She was, as she said, suffering from la grippe; grippe being then a new word in St. Petersburg, used only by the elite.\n","\n","All her invitations without exception, written in French, and delivered by a scarlet-liveried footman that morning, ran as follows:\n","\n","\"If you have nothing better to do, Count [or Prince], and if the prospect of spending an evening with a poor invalid is not too terrible, I shall be very charmed to see you tonight between 7 and 10—Annette Scherer.\"\n","\n","\"Heavens! what a virulent attack!\" replied the prince, not in the least disconcerted by this reception. He had just entered, wearing an embroidered court uniform, knee breeches, and shoes, and had stars on his breast and a serene expression on his flat face. He spoke in that refined French in which our grandfathers not only spoke but thought, and with the gentle, patronizing intonation natural to a man of importance who had grown old in society and at court. He went up to Anna Pavlovna, kissed her hand, presenting to her his bald, scented, and shining head, and complacently seated himself on the sofa.\n","\n","\"First of all, dear friend, tell me how you are. Set your friend’s mind at rest,\" said he without altering his tone, beneath the politeness and affected sympathy of which indifference and even irony could be discerned.\n","\n","\"Can one be well while suffering morally? Can one be calm in times like these if one has any feeling?\" said Anna Pavlovna. \"You are staying the whole evening, I hope?\"\n","\n","\"And the fete at the English ambassador’s? Today is Wednesday. I must put in an appearance there,\" said the prince. \"My daughter is coming for me to take me there.\"\n","\n","\"I thought today’s fete had been cancelled. I confess all these festivities and fireworks are becoming wearisome.\"\n","\n","\"If they had known that you wished it, the entertainment would have been put off,\" said the prince, who, like a wound-up clock, by force of habit said things he did not even wish to be believed.\n","\n","\"Don’t tease! Well, and what has been decided about Novosiltsev’s dispatch? You know everything.\"\n","\n","\"What can one say about it?\" replied the prince in a cold, listless tone. \"What has been decided? They have decided that Buonaparte has burnt his boats, and I believe that we are ready to burn ours.\"\n","\n","Prince Vasili always spoke languidly, like an actor repeating a stale part. Anna Pavlovna Scherer on the contrary, despite her forty years, overflowed with animation and impulsiveness. To be an enthusiast had become her social vocation and, sometimes even when she did not feel like it, she became enthusiastic in order not to disappoint the expectations of those who knew her. The subdued smile which, though it did not suit her faded features, always played round her lips expressed, as in a spoiled child, a continual consciousness of her charming defect, which she neither wished, nor could, nor considered it necessary, to correct.\n","\n","In the midst of a conversation on political matters Anna Pavlovna burst out:\n","\n","\"The Empress is pregnant again. Bonaparte will leave all Europe without monarchs. He evidently has conceived that the French are to be an exclusive people. Perhaps he has met his former wife and repented of his sins. To-day no one would be surprised at any change in his plans.\"\n","\n","\"Bonaparte seems to have forgotten that there are laws of political economy. Princes should always remember this. Here is a curious fact. When I was in London there were two women of rank, both of whom I knew personally, who gave birth to sons at precisely the same time. One was at the age of forty, the other at thirty. The first son was delicate, small, and weak; the second was a veritable Hercules. It was all the difference of ten years. Now if you please, the same thing holds true of Europe. Bonaparte forgets that the sovereigns who rule Europe are men like other men. And I dare say if you examined the matter, you would find that the nations that are ruled by these sovereigns are just as susceptible to diseases, to shocks, to all the influences that weaken and ruin men. And I am convinced that Bonaparte, the genius, the man who is so great a personage in his own estimation, is merely an instrument of Providence who is now wreaking vengeance upon the sovereigns of Europe. I can assure you he is no more than that.\"\n","\n","Anna Pavlovna paused in her tirade and looked at Prince Vasili with a questioning smile.\n","\n","\"Well, what do you think?\" she asked, seeing that he had not stirred a muscle. \"Is there anything in what I say?\"\"\"\n","\n","# Anna Pavlovna Scherer was maid of honour and favourite of the Empress Marya Fedorovna. She was suffering from la grippe, a new word in St. Petersburg, used only by the elite. Prince Vasili Kuragin was a man of high rank and importance.\n","# context = 'tehran is the capital of iran.'\n","\n","response = llm.generate_text(question, context)\n","print(\"response:\", response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yTtMcNRODtk","executionInfo":{"status":"ok","timestamp":1717802845173,"user_tz":-210,"elapsed":6231,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"e4dba630-7bfb-493e-b348-6c415f9e2533"},"execution_count":104,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"stream","name":"stdout","text":["response: you—sit down and tell me all the news.\"\n"]}]},{"cell_type":"markdown","source":["## free up memmory"],"metadata":{"id":"G12Zw0BtMu7o"}},{"cell_type":"code","source":["gc.collect()\n","torch.cuda.empty_cache()"],"metadata":{"id":"f2O7gPxqMn86","executionInfo":{"status":"ok","timestamp":1717802911038,"user_tz":-210,"elapsed":381,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":105,"outputs":[]},{"cell_type":"code","source":["llm.free_memory()"],"metadata":{"id":"Sm2Ch5NdvjRc","executionInfo":{"status":"ok","timestamp":1717802911643,"user_tz":-210,"elapsed":608,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":["# Collection"],"metadata":{"id":"LdexZkZ662Dt"}},{"cell_type":"code","source":["class Collection:\n","    def __init__(self, collection_name: str, transformer_type: str = 'all-MiniLM-L6-v2', load_online=False, save_transformer=False):\n","        self.client = Client(Settings())\n","        existing_collections = [col.name for col in self.client.list_collections()]\n","        if collection_name in existing_collections:\n","            self.client.delete_collection(collection_name)\n","        self.collection = self.client.get_or_create_collection(collection_name)\n","        self.vectorizer = self.load_sentence_transformer(transformer_type, load_online, save_transformer)\n","\n","    def load_sentence_transformer(self, transformer_type: str, load_online: bool, save_transformer: bool):\n","        transformer_path = f'{project_path}/models/{transformer_type}' if not load_online else transformer_type\n","        vectorizer = SentenceTransformer(transformer_path)\n","\n","        if save_transformer:\n","            vectorizer.save(f'{project_path}/models/{transformer_type}')\n","\n","        return vectorizer\n","\n","    def add_contexts(self, context_data: list):\n","        vectors = self.vectorizer.encode(context_data)\n","        ids = [f\"context_{i}\" for i in range(len(context_data))]\n","        self.collection.add(ids=ids, embeddings=vectors.tolist(), documents=context_data)\n","        print(\"Documents added to ChromaDB.\")\n","\n","    def retrieve_contexts(self, question: str, top_n: int = 1):\n","        question_vector = self.vectorizer.encode([question])[0].tolist()\n","        results = self.collection.query(query_embeddings=[question_vector], n_results=top_n)\n","        results = results['documents'][0]\n","        return results[:top_n]"],"metadata":{"id":"AQIdviiA6i4H","executionInfo":{"status":"ok","timestamp":1717802936037,"user_tz":-210,"elapsed":441,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":["## test Collection class"],"metadata":{"id":"ovlSNOQQwwPb"}},{"cell_type":"code","source":["# tranformer types:\n","\n","# default => all-MiniLM-L6-v2\n","# paraphrase-MiniLM-L6-v2\n","# paraphrase-xlm-r-multilingual-v1\n","# stsb-roberta-large"],"metadata":{"id":"DM5CKh11UQJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collection = Collection('rag')\n","\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","collection.add_contexts(context_data)"],"metadata":{"id":"FzCnp-Znw47j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = collection.retrieve_contexts('amazon', top_n=2)\n","\n","print(response)"],"metadata":{"id":"OYQlpAekWYzB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG"],"metadata":{"id":"c-HzDeeB69UN"}},{"cell_type":"code","source":["class RAG:\n","    def __init__(self, llm: LLM, collection: Collection):\n","        self.llm = llm\n","        self.collection = collection\n","\n","    def generate_response(self, query: str, top_n: int=1) -> str:\n","        retrieved_contexts = self.collection.retrieve_contexts(query, top_n)\n","        retrieved_contexts = '\\n'.join(retrieved_contexts)\n","        response = self.llm.generate_text(query, retrieved_contexts)\n","        return response\n"],"metadata":{"id":"pRN9IYwTHNAN","executionInfo":{"status":"ok","timestamp":1717802940631,"user_tz":-210,"elapsed":394,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":["## test RAG class"],"metadata":{"id":"sNCKr-Kaw0NY"}},{"cell_type":"code","source":["llm = GPT2()\n","collection = Collection('rag')\n","\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","collection.add_contexts(context_data)"],"metadata":{"id":"jYTZaTUOHOc8","executionInfo":{"status":"ok","timestamp":1717802968370,"user_tz":-210,"elapsed":13442,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d87dee0d-6a07-4c58-ac46-989af5534df6"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n"]}]},{"cell_type":"code","source":["rag = RAG(llm, collection)"],"metadata":{"id":"2VmxmObZHpeb","executionInfo":{"status":"ok","timestamp":1717802968371,"user_tz":-210,"elapsed":12,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["query = \"tell me about china?\"\n","response = rag.generate_response(query, top_n=3)\n","print(response)"],"metadata":{"id":"EqIbl7bPbXrh","executionInfo":{"status":"ok","timestamp":1717802975299,"user_tz":-210,"elapsed":6939,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49de2377-a0a9-42d9-f374-75771e38ba1e"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["It is the largest of the three major tropical regions.\n"]}]},{"cell_type":"markdown","source":["# Summarizer"],"metadata":{"id":"jQwuKM0jFmLz"}},{"cell_type":"code","source":["class Summarizer:\n","\n","    summarizer_models = {\n","        't5': (T5Tokenizer, T5ForConditionalGeneration, 't5-large'),\n","        'bart': (BartTokenizer, BartForConditionalGeneration, 'facebook/bart-large-cnn'),\n","        'pegasus': (PegasusTokenizer, PegasusForConditionalGeneration, 'google/pegasus-large'),\n","    }\n","\n","    def __init__(self, summarizer_model:str='t5', load_online=False, save_model=False):\n","        self.device = self.select_device()\n","        self.tokenizer, self.model = self.load_summarizer(summarizer_model, load_online, save_model)\n","        self.model.to(self.device)\n","        logging.basicConfig(level=logging.INFO)\n","        logging.info(f\"Model {summarizer_model} loaded and moved to {self.device}.\")\n","\n","    def load_summarizer(self, summarizer_model: str, load_online: bool, save_model: bool):\n","\n","        tokenizer_class, model_class, model_path = self.summarizer_models[summarizer_model]\n","\n","        if not load_online:\n","            model_path = f\"{project_path}/models/{model_path}\"\n","\n","        tokenizer = tokenizer_class.from_pretrained(model_path)\n","        model = model_class.from_pretrained(model_path)\n","\n","        if save_model:\n","            tokenizer.save_pretrained(f'{project_path}/models/{model_path}')\n","            model.save_pretrained(f'{project_path}/models/{model_path}')\n","\n","        return tokenizer, model\n","\n","    @staticmethod\n","    def select_device() -> str:\n","        return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    def summarize_text(self, input_text: str, context: str = '') -> str:\n","        raise NotImplementedError(\"The generate_text method should be implemented by the subclass.\")\n","\n","    def free_memory(self):\n","        del self.model\n","        del self.tokenizer\n","        gc.collect()\n","        torch.cuda.empty_cache()"],"metadata":{"id":"kUAjaPRIF2b7","executionInfo":{"status":"ok","timestamp":1717800646147,"user_tz":-210,"elapsed":380,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["class T5_Summarizer(Summarizer):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('t5', load_online, save_model)\n","\n","    def summarize_text(self, input_text: str) -> str:\n","        # model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","        # tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","        inputs = self.tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n","        max_length = min(len(input_text.split()), 150)  # Adjust max length based on input length\n","        min_length = min(len(input_text.split()) // 5, 40)  # Adjust min length based on input length\n","        summary_ids = self.model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n","        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","        return summary"],"metadata":{"id":"lUE9RMZR2Jq_","executionInfo":{"status":"ok","timestamp":1717800722560,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["from transformers import BartForConditionalGeneration, BartTokenizer\n","\n","def summarize_with_bart(text: str) -> str:\n","    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True)\n","    max_length = min(len(text.split()), 150)  # Adjust max length based on input length\n","    min_length = min(len(text.split()) // 5, 40)  # Adjust min length based on input length\n","    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary"],"metadata":{"id":"ZLE0LxmyHuWO","executionInfo":{"status":"ok","timestamp":1717797144399,"user_tz":-210,"elapsed":13894,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from transformers import T5ForConditionalGeneration, T5Tokenizer\n","\n","def summarize_with_t5(text: str) -> str:\n","    model = T5ForConditionalGeneration.from_pretrained('t5-large')\n","    tokenizer = T5Tokenizer.from_pretrained('t5-large')\n","    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors='pt', max_length=1024, truncation=True)\n","    max_length = min(len(text.split()), 150)  # Adjust max length based on input length\n","    min_length = min(len(text.split()) // 5, 40)  # Adjust min length based on input length\n","    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary\n","\n"],"metadata":{"id":"lBAvur5NHvq7","executionInfo":{"status":"ok","timestamp":1717797589762,"user_tz":-210,"elapsed":382,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","\n","def summarize_with_pegasus(text: str) -> str:\n","    model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-large')\n","    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-large')\n","    inputs = tokenizer(text, return_tensors='pt', max_length=1024, truncation=True)\n","    max_length = min(len(text.split()), 150)  # Adjust max length based on input length\n","    min_length = min(len(text.split()) // 5, 40)  # Adjust min length based on input length\n","    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summary\n"],"metadata":{"id":"HdDnYkaFHx4C","executionInfo":{"status":"ok","timestamp":1717798080521,"user_tz":-210,"elapsed":387,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["context = \"\"\"Well, Prince, so Genoa and Lucca are now just family estates of the Buonapartes. But I warn you, if you don’t tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist—I really believe he is Antichrist—I will have nothing more to do with you and you are no longer my friend, no longer my ‘faithful slave’, as you call yourself! But how do you do? I see I have frightened you—sit down and tell me all the news.\"\n","\n","It was in July, 1805, and the speaker was the well-known Anna Pavlovna Scherer, maid of honour and favourite of the Empress Marya Fedorovna. With these words she greeted Prince Vasili Kuragin, a man of high rank and importance, who was the first to arrive at her reception. Anna Pavlovna had had a cough for some days. She was, as she said, suffering from la grippe; grippe being then a new word in St. Petersburg, used only by the elite.\n","\n","All her invitations without exception, written in French, and delivered by a scarlet-liveried footman that morning, ran as follows:\n","\n","\"If you have nothing better to do, Count [or Prince], and if the prospect of spending an evening with a poor invalid is not too terrible, I shall be very charmed to see you tonight between 7 and 10—Annette Scherer.\"\n","\n","\"Heavens! what a virulent attack!\" replied the prince, not in the least disconcerted by this reception. He had just entered, wearing an embroidered court uniform, knee breeches, and shoes, and had stars on his breast and a serene expression on his flat face. He spoke in that refined French in which our grandfathers not only spoke but thought, and with the gentle, patronizing intonation natural to a man of importance who had grown old in society and at court. He went up to Anna Pavlovna, kissed her hand, presenting to her his bald, scented, and shining head, and complacently seated himself on the sofa.\n","\n","\"First of all, dear friend, tell me how you are. Set your friend’s mind at rest,\" said he without altering his tone, beneath the politeness and affected sympathy of which indifference and even irony could be discerned.\n","\n","\"Can one be well while suffering morally? Can one be calm in times like these if one has any feeling?\" said Anna Pavlovna. \"You are staying the whole evening, I hope?\"\n","\n","\"And the fete at the English ambassador’s? Today is Wednesday. I must put in an appearance there,\" said the prince. \"My daughter is coming for me to take me there.\"\n","\n","\"I thought today’s fete had been cancelled. I confess all these festivities and fireworks are becoming wearisome.\"\n","\n","\"If they had known that you wished it, the entertainment would have been put off,\" said the prince, who, like a wound-up clock, by force of habit said things he did not even wish to be believed.\n","\n","\"Don’t tease! Well, and what has been decided about Novosiltsev’s dispatch? You know everything.\"\n","\n","\"What can one say about it?\" replied the prince in a cold, listless tone. \"What has been decided? They have decided that Buonaparte has burnt his boats, and I believe that we are ready to burn ours.\"\n","\n","Prince Vasili always spoke languidly, like an actor repeating a stale part. Anna Pavlovna Scherer on the contrary, despite her forty years, overflowed with animation and impulsiveness. To be an enthusiast had become her social vocation and, sometimes even when she did not feel like it, she became enthusiastic in order not to disappoint the expectations of those who knew her. The subdued smile which, though it did not suit her faded features, always played round her lips expressed, as in a spoiled child, a continual consciousness of her charming defect, which she neither wished, nor could, nor considered it necessary, to correct.\n","\n","In the midst of a conversation on political matters Anna Pavlovna burst out:\n","\n","\"The Empress is pregnant again. Bonaparte will leave all Europe without monarchs. He evidently has conceived that the French are to be an exclusive people. Perhaps he has met his former wife and repented of his sins. To-day no one would be surprised at any change in his plans.\"\n","\n","\"Bonaparte seems to have forgotten that there are laws of political economy. Princes should always remember this. Here is a curious fact. When I was in London there were two women of rank, both of whom I knew personally, who gave birth to sons at precisely the same time. One was at the age of forty, the other at thirty. The first son was delicate, small, and weak; the second was a veritable Hercules. It was all the difference of ten years. Now if you please, the same thing holds true of Europe. Bonaparte forgets that the sovereigns who rule Europe are men like other men. And I dare say if you examined the matter, you would find that the nations that are ruled by these sovereigns are just as susceptible to diseases, to shocks, to all the influences that weaken and ruin men. And I am convinced that Bonaparte, the genius, the man who is so great a personage in his own estimation, is merely an instrument of Providence who is now wreaking vengeance upon the sovereigns of Europe. I can assure you he is no more than that.\"\n","\n","Anna Pavlovna paused in her tirade and looked at Prince Vasili with a questioning smile.\n","\n","\"Well, what do you think?\" she asked, seeing that he had not stirred a muscle. \"Is there anything in what I say?\"\"\"\n","\n","summarizer = T5_Summarizer(load_online=True)\n","summary = summarizer.summarize_text(context)\n","print(summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mx1GETenIlr_","executionInfo":{"status":"ok","timestamp":1717801014948,"user_tz":-210,"elapsed":101112,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"8a6d92e4-0547-4419-98d4-fbf7a18af536"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":[": \"The Empress is pregnant again. Bonaparte will leave all Europe without monarchs. Perhaps he has met his former wife and repented.\" \"The Empress is pregnant again.\" \"The Empress is pregnant again.\" \"The Empress is pregnant again.\" \"The Empress is pregnant again.\" \"The Empress is pregnant again.\" \"The Empress is pregnant\n"]}]},{"cell_type":"markdown","source":["# PDF"],"metadata":{"id":"0ylfgAmLrWjQ"}},{"cell_type":"code","source":["def extract_text_from_pdf(pdf_path):\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page_num in range(len(doc)):\n","        page = doc.load_page(page_num)\n","        text += page.get_text(\"text\")\n","    return text\n"],"metadata":{"id":"21pFJVfPrcFH","executionInfo":{"status":"ok","timestamp":1717803161000,"user_tz":-210,"elapsed":379,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":112,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text, chunk_size=500):\n","    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n","    sentences = text.split('. ')  # Split into sentences\n","    chunks = []\n","    chunk = []\n","    length = 0\n","\n","    for sentence in sentences:\n","        chunk.append(sentence)\n","        length += len(sentence.split())\n","\n","        if length > chunk_size:\n","            chunks.append(' '.join(chunk))\n","            chunk = []\n","            length = 0\n","\n","    if chunk:\n","        chunks.append(' '.join(chunk))\n","\n","    return chunks\n"],"metadata":{"id":"78axRor7rfev","executionInfo":{"status":"ok","timestamp":1717803161932,"user_tz":-210,"elapsed":365,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["pdf_path = f\"{project_path}/tests/micro led 1.pdf\"\n","text = extract_text_from_pdf(pdf_path)\n","contexts = preprocess_text(text)"],"metadata":{"id":"4Q6xtFE3rgOI","executionInfo":{"status":"ok","timestamp":1717803172140,"user_tz":-210,"elapsed":4207,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["collection = Collection('rag')\n","collection.add_contexts(contexts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXAVjf3mud5I","executionInfo":{"status":"ok","timestamp":1717803178840,"user_tz":-210,"elapsed":6702,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"fd5dc0cd-1976-47a0-b8df-36ea1ca003b3"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n"]}]},{"cell_type":"code","source":["# llm = NeoGPT(load_online=True)\n","# llm = BERT()\n","llm = GPT2()"],"metadata":{"id":"f5zc3ii4ufPI","executionInfo":{"status":"ok","timestamp":1717803180014,"user_tz":-210,"elapsed":1179,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["rag = RAG(llm, collection)"],"metadata":{"id":"x7TmnoW0uqeJ","executionInfo":{"status":"ok","timestamp":1717803180014,"user_tz":-210,"elapsed":4,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":["question = 'what is One of the most widely anticipated applications of microLEDs?'\n","real_context = \"One of the most widely anticipated applications of microLEDs and RGB capability is in next-generation self-emissive display technol-ogy.10,11\"\n","\n","# question = 'what we should expect from adoption of microled displays?'\n","# real_context = \"The adoption of microLEDs in display devices is expected to enable much brighter, broader color gamut, longer lifetime, and extremely high pixels per inch (PPI) displays.\"\n","\n","response = rag.generate_response(question, 5)\n","\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Owx0UKj0uwMX","executionInfo":{"status":"ok","timestamp":1717803387646,"user_tz":-210,"elapsed":9692,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"32c84899-9a9a-4929-a884-5ca588d0853f"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["This paper aims to answer a question that has been raised by many researchers regarding the potential use of microLEDs for biomedical applications. One of the major problems with microLEDs is that they have a limited range of uses and applications. For example, they can be used to light the body, provide a light source for the retina, or to be used to monitor other applications, such as a doctor\n"]}]}]}