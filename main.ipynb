{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"mount_file_id":"1xGeaM-oFVeEliD38KtLRTtP4lPrQVkz-","authorship_tag":"ABX9TyOugPyXZm8CtOpEwqvTJL5Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"XyuFSQGWlMrD"}},{"cell_type":"code","source":["!pip install -q chromadb sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xxPdB5Lm6zpx","executionInfo":{"status":"ok","timestamp":1717174140922,"user_tz":-210,"elapsed":107246,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"b82f3844-56d4-4b64-b10e-e3d6bbf67378"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","import logging\n","from transformers import (\n","    AutoModelForCausalLM, AutoTokenizer,\n","    GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration,\n","    BertTokenizer, BertForMaskedLM, DistilBertTokenizer, DistilBertForMaskedLM,\n","    RobertaTokenizer, RobertaForMaskedLM, GPTNeoForCausalLM\n",")\n","import torch\n","from chromadb import Client, Settings\n","from sentence_transformers import SentenceTransformer"],"metadata":{"id":"mJQSWTpCxjb2","executionInfo":{"status":"ok","timestamp":1717174145837,"user_tz":-210,"elapsed":4921,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/Colab Notebooks/RAG'"],"metadata":{"id":"5wT2fvm20bmm","executionInfo":{"status":"ok","timestamp":1717174145837,"user_tz":-210,"elapsed":17,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# GPU"],"metadata":{"id":"-51Pl889lKDn"}},{"cell_type":"code","source":["\n","print(torch.cuda.is_available())\n","# print(torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zd5YQ-IfoDFN","executionInfo":{"status":"ok","timestamp":1717151778903,"user_tz":-210,"elapsed":3,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"f5b54f85-ad84-4ecc-8d00-7190496c8fe3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["# Hugging Face Login"],"metadata":{"id":"dPnOa3oLlRy2"}},{"cell_type":"code","source":["# Log in using your Hugging Face access token\n","access_token = \"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\"\n","login(access_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QvoS3g0665l","executionInfo":{"status":"ok","timestamp":1717174145838,"user_tz":-210,"elapsed":17,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"990076ee-b823-49d9-aa9b-3ae09b1aeb6a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# LLM\n","\n","\n","---\n","\n","###key functions of LLM class:\n","\n","\n","*   **load_llm_local**: tries to load an llm from google drvie\n","*   **load_llm_online**: loads the llm from hugging face\n","*   **select_device**: if gpu is available it will select it\n","*   **generate_text**: it can generate text based on given prompt"],"metadata":{"id":"_com4NubP5Jv"}},{"cell_type":"code","source":["class LLM:\n","    def __init__(self, llm_type: str, load_online=False, configs=None):\n","        self.configs = configs\n","        self.device = self.select_device()\n","        if not load_online:\n","            self.tokenizer, self.model = self.load_llm_local(llm_type)\n","        else:\n","            self.tokenizer, self.model = self.load_llm_online(llm_type)\n","        self.model.to(self.device)\n","        logging.basicConfig(level=logging.INFO)\n","        logging.info(f\"Model {llm_type} loaded and moved to {self.device}.\")\n","\n","    def load_llm_local(self, llm_type: str):\n","        try:\n","            if llm_type == 'gpt2':\n","                model_path = f\"{project_path}/models/gpt2\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPT2LMHeadModel.from_pretrained(model_path)\n","            elif llm_type == 'distilgpt2':\n","                model_path = f\"{project_path}/models/distilgpt2\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPT2LMHeadModel.from_pretrained(model_path)\n","            elif llm_type == 't5-small':\n","                model_path = f\"{project_path}/models/t5-small\"\n","                tokenizer = T5Tokenizer.from_pretrained(model_path)\n","                model = T5ForConditionalGeneration.from_pretrained(model_path)\n","            elif llm_type == 'bert':\n","                model_path = f\"{project_path}/models/bert\"\n","                tokenizer = BertTokenizer.from_pretrained(model_path)\n","                model = BertForMaskedLM.from_pretrained(model_path)\n","            elif llm_type == 'distilbert':\n","                model_path = f\"{project_path}/models/distilbert\"\n","                tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n","                model = DistilBertForMaskedLM.from_pretrained(model_path)\n","            elif llm_type == 'roberta':\n","                model_path = f\"{project_path}/models/roberta\"\n","                tokenizer = RobertaTokenizer.from_pretrained(model_path)\n","                model = RobertaForMaskedLM.from_pretrained(model_path)\n","            elif llm_type == 'gpt-neo':\n","                model_path = f\"{project_path}/models/gpt-neo\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPTNeoForCausalLM.from_pretrained(model_path)\n","            else:\n","                raise ValueError(f\"Unsupported model type: {llm_type}\")\n","            return tokenizer, model\n","        except Exception as e:\n","            logging.error(f\"Error loading model {llm_type} from {model_path}: {e}\")\n","            raise\n","    def load_llm_online(self, llm_type: str):\n","        try:\n","            if llm_type == 'gpt2':\n","                model_path = \"gpt2\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPT2LMHeadModel.from_pretrained(model_path)\n","\n","            elif llm_type == 'distilgpt2':\n","                model_path = \"distilgpt2\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPT2LMHeadModel.from_pretrained(model_path)\n","\n","            elif llm_type == 't5-small':\n","                model_path = \"t5-small\"\n","                tokenizer = T5Tokenizer.from_pretrained(model_path)\n","                model = T5ForConditionalGeneration.from_pretrained(model_path)\n","\n","            elif llm_type == 'bert':\n","                model_path = \"bert-base-uncased\"\n","                tokenizer = BertTokenizer.from_pretrained(model_path)\n","                model = BertForMaskedLM.from_pretrained(model_path)\n","\n","            elif llm_type == 'distilbert':\n","                model_path = \"distilbert-base-uncased\"\n","                tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n","                model = DistilBertForMaskedLM.from_pretrained(model_path)\n","\n","            elif llm_type == 'roberta':\n","                model_path = \"roberta-base\"\n","                tokenizer = RobertaTokenizer.from_pretrained(model_path)\n","                model = RobertaForMaskedLM.from_pretrained(model_path)\n","\n","            elif llm_type == 'gpt-neo':\n","                model_path = \"EleutherAI/gpt-neo-125M\"\n","                tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","                model = GPTNeoForCausalLM.from_pretrained(model_path)\n","\n","            else:\n","                raise ValueError(f\"Unsupported model type: {llm_type}\")\n","\n","            # save the model after download\n","            tokenizer.save_pretrained(f'{project_path}/models/{llm_type}')\n","            model.save_pretrained(f'{project_path}/models/{llm_type}')\n","            return tokenizer, model\n","        except Exception as e:\n","            logging.error(f\"Error downloading model {llm_type} from {model_path}: {e}\")\n","            raise\n","\n","    @staticmethod\n","    def select_device() -> str:\n","        return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    def generate_text(self, input_text: str) -> str:\n","        try:\n","            input_ids = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)\n","            if self.tokenizer.pad_token_id is None:\n","                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n","            attention_mask = input_ids.ne(self.tokenizer.pad_token_id).long().to(self.device)\n","\n","            default_configs = {\n","                'max_length': 100,\n","                'num_return_sequences': 1,\n","                'do_sample': True,\n","                'top_k': 90,\n","                'top_p': 0.95,\n","                'temperature': 0.3,\n","                'attention_mask': attention_mask,\n","                'pad_token_id': self.tokenizer.eos_token_id\n","            }\n","\n","            if self.configs:\n","                default_configs.update(self.configs)\n","\n","            output = self.model.generate(input_ids, **default_configs)\n","\n","            return self.tokenizer.decode(output[0], skip_special_tokens=True)\n","        except Exception as e:\n","            logging.error(f\"Error generating text for input '{input_text}': {e}\")\n","            return \"Error generating text.\"\n"],"metadata":{"id":"DRAdIqF3kqJo","executionInfo":{"status":"ok","timestamp":1717174145838,"user_tz":-210,"elapsed":14,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["llm = LLM('gpt2', )"],"metadata":{"id":"gQmSl5i7NxZB","executionInfo":{"status":"ok","timestamp":1717160679333,"user_tz":-210,"elapsed":4264,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["output = llm.generate_text('question: who are you, answer is mohammad')\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aUr-A-15OYDD","executionInfo":{"status":"ok","timestamp":1717160731075,"user_tz":-210,"elapsed":10561,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"a95aaa04-2bdf-451c-d262-b24dc7e8c694"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["question: who are you, answer is mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am mohammad.\n","\n","Answer: I am moh\n"]}]},{"cell_type":"markdown","source":["# Collection"],"metadata":{"id":"LdexZkZ662Dt"}},{"cell_type":"code","source":["class Collection:\n","    def __init__(self, collection_name: str, model_name: str = 'all-MiniLM-L6-v2', load_online=False):\n","        self.client = Client(Settings())\n","        existing_collections = [col.name for col in self.client.list_collections()]\n","        if collection_name in existing_collections:\n","            self.client.delete_collection(collection_name)\n","        existing_collections = [col.name for col in self.client.list_collections()]\n","        self.collection = self.client.get_or_create_collection(collection_name)\n","        if not load_online:\n","            self.vectorizer = SentenceTransformer(f'{project_path}/models/{model_name}')\n","        else:\n","            self.vectorizer = SentenceTransformer(model_name)\n","            self.vectorizer.save(f'{project_path}/models/{model_name}')\n","\n","    def add_contexts(self, context_data: list):\n","        vectors = self.vectorizer.encode(context_data)\n","        ids = [f\"context_{i}\" for i in range(len(context_data))]\n","        self.collection.add(ids=ids, embeddings=vectors.tolist(), documents=context_data)\n","        print(\"Documents added to ChromaDB.\")\n","\n","    def retrieve_context(self, question: str):\n","        question_vector = self.vectorizer.encode([question])[0].tolist()\n","        results = self.collection.query(query_embeddings=[question_vector], n_results=1)\n","        return results['documents'][0]"],"metadata":{"id":"AQIdviiA6i4H","executionInfo":{"status":"ok","timestamp":1717176234621,"user_tz":-210,"elapsed":352,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='all-MiniLM-L6-v2')\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='paraphrase-MiniLM-L6-v2')\n","collection = Collection(collection_name=\"qa_contexts\", model_name='paraphrase-xlm-r-multilingual-v1')\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='stsb-roberta-large')\n","\n","# Add contexts to the collection\n","collection.add_contexts(context_data)\n","\n","# Retrieve a context based on a question\n","question = \"What is the capital of France?\"\n","context = collection.retrieve_context(question)\n","print(f\"Retrieved context: {context}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoDt7bZjB_kI","executionInfo":{"status":"ok","timestamp":1717176261639,"user_tz":-210,"elapsed":9242,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"5feb2af1-d9f7-4b81-e20a-e5c922375c86"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n","Retrieved context: ['The capital of France is Paris. It is known for its art, culture, and cuisine.']\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","class BERTBasedModel:\n","    def __init__(self):\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    def encode_text(self, text):\n","        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n","        outputs = self.model(**inputs)\n","        return outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n","\n","# Usage example\n","if __name__ == \"__main__\":\n","    bert_model = BERTBasedModel()\n","    text = \"The capital of France is Paris.\"\n","    encoded_text = bert_model.encode_text(text)\n","    print(encoded_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BwbNqt-779Q","executionInfo":{"status":"ok","timestamp":1717163776147,"user_tz":-210,"elapsed":4105,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"31246917-d85b-4c4f-e77e-936e4163a067"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["[-2.13676035e-01 -3.91903222e-01 -2.66700089e-01 -3.90870571e-02\n"," -3.93290162e-01 -9.54794586e-02  8.90238136e-02  1.03771603e+00\n"," -2.21996307e-01 -2.70034432e-01 -3.49999458e-01 -4.28629965e-01\n"," -7.70025030e-02  1.71783954e-01 -1.31053180e-01 -1.23675130e-02\n"," -1.39211595e-01  1.52360499e-01  3.39183770e-02 -4.00302485e-02\n"," -2.77385622e-01  4.52033371e-01  4.41524871e-02  4.77774471e-01\n","  5.29628515e-01 -6.12427406e-02 -2.49680728e-01  5.03534414e-02\n","  5.06222434e-02 -2.36037657e-01  3.21265273e-02  1.99337065e-01\n"," -2.26213992e-01 -6.91059530e-02  5.07191896e-01  9.36603248e-02\n","  2.01888323e-01 -6.04904629e-02 -1.24802321e-01  1.13808803e-01\n"," -2.64049262e-01 -6.49127007e-01  1.19680561e-01 -2.82466352e-01\n"," -2.95890179e-02 -4.63031411e-01  1.74842656e-01  5.03693402e-01\n","  5.76818287e-01  2.61461318e-01 -6.11727893e-01  3.30407768e-01\n"," -9.77744162e-03  4.70626354e-01  6.99345469e-01  7.66546547e-01\n"," -4.58161354e-01 -5.63782990e-01 -7.32800007e-01  1.02959208e-01\n"," -1.81162879e-01  3.28731239e-01  1.34030044e-01  1.00399218e-02\n","  2.34354764e-01  6.05888844e-01  2.57624090e-01  2.51111835e-01\n"," -4.39216286e-01 -5.93088455e-02 -8.02473724e-01 -3.78958046e-01\n"," -8.23131427e-02 -9.14423838e-02 -2.71013618e-01 -1.74355641e-01\n"," -6.05620384e-01  5.47059774e-01 -2.83327222e-01 -2.46823817e-01\n","  8.09502825e-02  5.65362632e-01  6.03040814e-01  6.55181646e-01\n"," -2.03273371e-01  3.03601921e-01 -4.19250667e-01  4.91572358e-02\n"," -6.24049380e-02  2.89062589e-01 -2.61148244e-01 -1.55700356e-01\n"," -7.44050369e-02  1.00698486e-01  1.17830038e-01 -2.46033132e-01\n","  4.69244048e-02 -8.90856534e-02 -1.50701120e-01  5.99683642e-01\n","  4.86846827e-02 -8.35483223e-02 -6.75706044e-02  5.29303312e-01\n","  3.87885869e-02  5.74721908e-03 -5.41427284e-02 -2.72395730e-01\n","  1.59889355e-01  3.18747669e-01  5.75890392e-02 -5.64584672e-01\n","  5.30160815e-02 -3.16889077e-01 -7.76392296e-02  3.11324596e-01\n","  2.80003190e-01 -1.55696616e-01 -7.14626834e-02 -8.46895650e-02\n"," -2.50836462e-01 -1.47627726e-01 -8.77281204e-02  9.22783017e-01\n"," -3.19897950e-01 -2.70026833e-01  2.61980951e-01  3.44450474e-01\n"," -2.46420622e-01 -5.42660952e-01  1.04072556e-01  2.24635527e-01\n"," -3.34214658e-01 -3.51844877e-01 -2.04646006e-01  3.61773185e-02\n","  1.81041926e-01  2.91437387e-01 -5.04750535e-02  2.70377934e-01\n","  3.04966122e-01 -9.32052732e-03  9.64152515e-02 -4.69458044e-01\n","  1.84530690e-01 -1.15477480e-02 -1.29333269e-02 -2.61131048e-01\n","  1.46575809e-01 -1.88732281e-01  4.13194448e-01  2.40011632e-01\n","  1.64925516e-01 -4.90368381e-02 -3.95212471e-01 -5.80783486e-01\n","  1.76549882e-01  2.77643204e-01  6.44997776e-01  2.75237024e-01\n","  1.08310856e-01 -4.22874808e-01  5.30469656e-01  1.38413921e-01\n"," -2.95977354e-01  4.38028246e-01  2.05585897e-01  7.37760782e-01\n","  3.22439000e-02  3.21136832e-01 -6.71881974e-01 -1.35425866e-01\n","  9.96138692e-01 -3.39250684e-01  4.23520237e-01 -7.58130103e-02\n","  5.05404267e-03  3.49069506e-01  2.09784508e-01  1.41712084e-01\n"," -1.94640303e+00  2.16580808e-01  3.61758113e-01  2.07247630e-01\n","  4.33325022e-01  2.00865760e-01  7.77777135e-02 -4.07279849e-01\n"," -3.01262408e-01 -1.01787644e-02 -1.01011038e-01 -2.07048431e-02\n"," -7.71634519e-01 -6.07236207e-01 -1.58528283e-01 -6.33372724e-01\n"," -1.86923981e-01 -9.76605788e-02 -4.01609130e-02  1.08806238e-01\n"," -5.33934891e-01 -3.56470555e-01  5.04741371e-01  4.82360385e-02\n"," -3.50192994e-01 -4.48631570e-02 -5.39036036e-01 -5.21573424e-01\n"," -2.79869884e-03  5.76556504e-01 -1.22399040e-01  5.46167567e-02\n"," -6.56983927e-02 -1.46803617e-01 -3.51426117e-02 -3.12288076e-01\n"," -5.23988843e-01 -3.54337811e-01 -3.48423719e-01 -2.73419023e-01\n"," -4.78237867e-02 -3.20977747e-01 -3.21376204e-01  2.60279536e-01\n"," -2.86818624e-01  5.91241777e-01  3.78824294e-01  3.74970019e-01\n"," -4.21766639e-01  2.24083349e-01 -2.27916509e-01 -3.94489586e-01\n","  1.50946215e-01  6.01596087e-02 -1.02490354e+00  2.38065273e-01\n"," -7.55678594e-01 -3.81839633e-01  1.39379188e-01 -6.18476868e-01\n","  2.39914790e-01 -4.10265140e-02  5.40370286e-01 -6.76657781e-02\n","  1.24111719e-01 -4.63682413e-03 -3.20119590e-01 -3.67891528e-02\n","  4.42347646e-01  5.71746156e-02 -8.94952118e-02 -9.93910074e-01\n","  2.45039701e-01 -3.04696351e-01  1.42257474e-02 -2.58145958e-01\n","  3.18797827e-02  3.65538411e-02  4.05264646e-01  3.59193683e-01\n"," -1.05687633e-01 -1.55132055e-01  1.68441772e-01  2.72994101e-01\n"," -6.61434472e-01 -2.07085848e-01  1.51906863e-01 -5.39315399e-03\n","  1.31226838e-01  1.22813515e-01 -5.48300028e-01 -2.91214585e-01\n"," -1.12866178e-01  2.49558553e-01 -2.55948842e-01  6.51560724e-02\n","  4.42767501e-01  3.33174318e-01  5.29950075e-02  3.55542630e-01\n","  4.82850671e-02  9.95297730e-01 -8.06703925e-01 -1.36245832e-01\n"," -2.08117738e-01 -1.16176523e-01  3.16693261e-02 -2.18430683e-01\n"," -1.88035637e-01 -3.05944860e-01 -8.18984330e-01 -2.28881538e-01\n"," -8.13440233e-02  1.02958828e-01  4.06313539e-02 -5.75963795e-01\n","  3.57946396e-01  4.79890883e-01 -4.08058316e-02 -3.45562935e-01\n"," -3.28736484e-01 -4.77816105e-01  7.14796305e-01  1.69047773e-01\n","  4.58633065e-01  1.40755296e-01  1.22151934e-01 -6.75569475e-02\n"," -1.26797414e+00  2.11202055e-01  1.28766656e-01 -4.18562770e-01\n","  3.04463774e-01  9.45184678e-02  2.08537161e-01 -2.56614804e-01\n"," -3.53889823e-01 -2.72095263e-01 -4.56903502e-02 -2.52804339e-01\n","  3.64984363e-01  2.29996040e-01  3.55714589e-01  3.01016331e-01\n"," -3.07829976e-02 -5.31346679e-01 -2.35177919e-01  7.19491541e-01\n"," -2.07858026e-01 -2.28932887e-01  2.66511977e-01 -1.81899250e-01\n","  6.23579562e-01  1.78551838e-01 -4.41962391e-01 -4.64206487e-01\n"," -2.81293213e-01 -2.78444022e-01 -3.72808725e-01 -2.45529830e-01\n","  3.14039022e-01 -1.11794740e-01  4.88363719e-03 -2.05522478e-01\n","  8.01383853e-02  2.12772056e-01 -2.00275972e-01 -6.38398409e-01\n"," -1.02019846e-01  9.28504467e-02  6.18415233e-03 -4.64891978e-02\n","  6.88816190e-01  2.33140439e-01 -4.25053835e-01 -3.57336283e-01\n"," -1.93144858e-01  6.55115187e-01  4.52683091e-01 -9.51772034e-02\n"," -2.59104073e-01 -1.88865826e-01 -2.43808910e-01 -7.64657795e-01\n","  4.68646377e-01  4.81825352e-01 -7.30880499e-02 -5.50113134e-02\n","  2.43580237e-01  8.87094885e-02 -4.42488283e-01  7.14283511e-02\n","  2.38383666e-01 -4.10703450e-01 -8.15800905e-01  4.69996125e-01\n","  7.09637552e-02  5.48775196e-01 -1.93798468e-01  1.54108912e-01\n"," -1.06766331e+00 -5.31738162e-01 -6.74368918e-01 -3.54093134e-01\n","  2.46085346e-01  1.05221123e-01  3.92042696e-01  1.54805720e-01\n"," -1.68479905e-01  1.36976182e-01 -2.52649009e-01  6.55924678e-02\n"," -2.38273084e-01 -4.22452867e-01 -1.44410320e-02  1.72589391e-01\n"," -5.24264760e-03 -4.12480414e-01 -1.06518298e-01  3.74830037e-01\n","  6.43331289e-01  2.57728696e-01 -3.16212237e-01 -3.64744306e-01\n","  1.95565168e-03 -2.32083380e-01  1.10473506e-01  1.25154763e-01\n"," -3.02903295e-01 -1.21928215e-01  2.78192133e-01  4.02676798e-02\n","  1.85163841e-01  2.07295448e-01 -6.40448272e-01  4.33409959e-01\n","  8.87389630e-02  3.08886111e-01 -3.11433822e-01 -9.32105184e-01\n","  2.47492731e-01 -5.49991369e-01  5.21454811e-01 -7.57505178e-01\n","  4.88358229e-01  8.66458356e-01 -2.88972139e-01  3.90647560e-01\n"," -1.14534415e-01  8.01240683e-01 -3.12078893e-02 -8.67094398e-02\n"," -2.89959013e-01 -1.63649619e-01 -2.03677073e-01 -4.89905208e-01\n","  5.29201031e-02  4.02189121e-02 -2.39496917e-01  6.95752203e-02\n","  3.99623960e-01 -3.79619002e-02  3.40220988e-01  3.18999857e-01\n"," -1.48736060e-01 -2.71654814e-01 -5.02853617e-02  6.09017074e-01\n","  5.39785564e-01  4.50447023e-01 -4.53594178e-02  9.81611982e-02\n","  2.74319798e-01  2.08896711e-01  6.23738766e-01  1.29465640e-01\n","  5.31822026e-01  1.25424281e-01 -3.58162336e-02  5.62656263e-04\n","  1.72310054e-01 -1.09106407e-01  2.46300735e-02 -2.82586724e-01\n","  3.30700666e-01  1.25898020e-02  1.29041493e-01 -6.76336467e-01\n"," -1.01243824e-01 -1.39879286e-01  1.30201653e-01  6.52091742e-01\n","  3.89378965e-02  4.06874835e-01  1.19917683e-01 -4.29301471e-01\n"," -3.93478841e-01 -3.44812334e-01  4.60142076e-01 -1.38598979e-01\n","  1.11904465e-01 -5.30861378e-01 -4.31040227e-01  1.34364262e-01\n","  6.11017883e-01 -7.62338415e-02 -1.73964679e-01  2.47712247e-02\n","  2.13350251e-01 -3.91741604e-01 -1.45255523e-02  6.98934868e-02\n"," -2.51399606e-01  4.15309548e-01  1.98928833e-01 -2.92569518e-01\n","  6.80322647e-01 -2.12034523e-01 -3.78854036e-01  5.43167710e-01\n"," -1.00333858e+00 -9.79636759e-02 -1.00522888e+00 -5.24012208e-01\n","  1.70252457e-01 -3.82989585e-01  2.40778148e-01  6.09634042e-01\n"," -5.29536903e-02  2.06514560e-02  1.87896073e-01  9.41448882e-02\n"," -1.67888448e-01  1.41351238e-01 -2.43190564e-02 -2.38673687e-01\n"," -3.56549114e-01  1.82656929e-01 -8.20475280e-01 -4.13603008e-01\n"," -2.93290436e-01 -1.12198949e-01  2.37394512e-01 -4.18945625e-02\n","  4.24113005e-01 -3.83944154e-01 -1.25043869e-01  1.21159330e-01\n"," -6.09045684e-01 -5.71963310e-01  6.86931551e-01 -4.24809605e-02\n"," -5.06857932e-01  1.75592080e-01 -2.67552197e-01  3.40920538e-01\n"," -6.37644768e-01 -5.01473546e-01 -2.13289171e-01  2.08342776e-01\n"," -3.79723430e-01 -2.63710916e-01  5.21629989e-01  2.25281030e-01\n"," -8.60206038e-02 -4.80105937e-01 -1.24972500e-01 -3.21608692e-01\n","  3.07511594e-02 -1.08966559e-01  3.56892943e-01 -3.45145434e-01\n","  2.53610492e-01 -1.72461092e-01  1.14733525e-01  5.67501545e-01\n"," -3.17081660e-01  1.16936341e-01 -4.16158795e-01  1.96494758e-01\n","  9.45025310e-02 -5.19205689e-01 -5.68181753e-01 -4.26064640e-01\n"," -3.17987502e-01  1.68176040e-01  1.52208582e-01  3.30676466e-01\n","  9.49500129e-02 -2.10067928e-01  1.96181297e-01  5.05629778e-01\n"," -6.68740571e-02 -3.13717484e-01 -2.24899769e-01  5.10418892e-01\n","  1.02509767e-01 -3.52037758e-01  1.61760256e-01  1.46796122e-01\n"," -2.28234604e-01  5.72660901e-02 -2.80760407e-01  3.54718119e-01\n","  1.15825430e-01  3.33214775e-02  1.52710736e-01 -2.38063917e-01\n","  2.45703980e-02  7.81345189e-01  2.00289100e-01 -4.86134112e-01\n","  6.09368443e-01 -3.91065329e-01 -5.23833334e-01  8.40659559e-01\n"," -5.40243864e-01  5.85070178e-02  4.23098892e-01 -1.47252306e-01\n","  3.89558263e-02  4.85469073e-01  5.63001893e-02 -2.27992907e-01\n"," -6.05866313e-02  4.53197151e-01 -7.68160224e-02  6.42774701e-01\n"," -5.72315753e-01  4.43918437e-01  5.76241791e-01 -2.73645490e-01\n","  2.71834403e-01  1.71622530e-01 -2.04162791e-01 -1.52334139e-01\n","  8.05502892e-01  4.55253541e-01 -1.78627953e-01 -3.82404387e-01\n","  4.91134971e-02  6.35325193e-01 -7.45806634e-01  1.99971735e-01\n"," -2.32675821e-01 -8.16401184e-01 -5.44563383e-02  2.89525241e-01\n"," -1.80957213e-01 -1.99942484e-01 -8.15020688e-03  1.79381028e-01\n","  3.19152385e-01  1.07518697e+00 -3.22030127e-01 -1.67260662e-01\n","  2.39301413e-01 -2.72191986e-02 -2.23477662e-01  6.80233300e-01\n","  4.40692455e-02  7.16322660e-01 -5.66136777e-01 -4.37091231e-01\n"," -1.27355754e-01  5.48148394e-01  2.51585096e-01 -2.71887571e-01\n","  6.86513722e-01  3.52195054e-01 -4.41361442e-02  6.38234138e-01\n","  4.35796529e-01  1.48201466e-01 -4.11945134e-02 -3.82172167e-01\n","  3.90739173e-01 -2.83374518e-01  5.26066065e-01  6.84068128e-02\n"," -4.27614897e-02 -9.03799534e-02  2.64801234e-01 -1.81555852e-01\n","  5.03642619e-01  1.30679652e-01  3.22251469e-02  1.64835215e-01\n","  3.62783670e-01 -2.29120567e-01  4.47028428e-01  1.13691434e-01\n","  2.55286336e-01  4.97256696e-01  1.99107453e-01 -3.42126727e-01\n"," -4.86535698e-01 -1.09734774e-01 -1.33819759e-01  3.29611510e-01\n","  3.63562286e-01 -3.14886212e-01  4.37997639e-01  5.42970896e-01\n"," -4.23104018e-01 -1.31995142e-01 -9.02435929e-02 -4.26101804e-01\n"," -1.62672266e-01 -6.82771981e-01 -5.43061554e-01 -3.30092728e-01\n"," -3.81466508e-01  3.29708993e-01 -2.81416476e-01 -7.67227054e-01\n","  3.23955081e-02 -1.09972805e-01  3.24335575e-01 -2.46083677e-01\n","  1.45483404e-01 -5.57969689e-01  5.05297422e-01 -6.81001484e-01\n","  3.36872131e-01  3.39583606e-01  2.54889101e-01 -2.30673954e-01\n","  9.48693573e-01 -4.62221950e-01 -6.53899610e-01  6.32907301e-02\n","  4.47888583e-01  2.24926308e-01  2.18057811e-01 -1.18998326e-02\n"," -5.21143317e-01 -4.36382025e-01 -1.55054152e-01  2.32214898e-01\n"," -1.02053773e+00  6.21557951e-01  3.05926919e-01  4.87771705e-02\n","  1.16301090e-01 -3.06955278e-02  1.53965965e-01  1.12775341e-01\n"," -1.91100836e-02 -4.13260102e-01  3.47211212e-02  5.61828017e-01\n"," -1.78061113e-01 -6.57314301e-01  3.07962060e-01  7.78799593e-01\n","  2.51815706e-01 -2.04027846e-01 -2.25845262e-01  2.05490395e-01\n","  1.15376130e-01 -3.31272244e-01  1.32484585e-01 -3.38987797e-01\n"," -2.65015569e-02  3.95999879e-01 -7.02565730e-01 -8.31795037e-01\n","  1.78852584e-02  2.55966395e-01 -1.76609173e-01 -1.58798009e-01\n"," -9.08671498e-01 -4.37335432e-01 -6.78260088e-01 -3.21590960e-01\n","  1.66854933e-01 -2.89071083e-01  2.63371199e-01  1.39496610e-01\n","  1.30863249e-01 -2.40898788e-01  3.72371256e-01 -6.39544502e-02\n","  2.69994438e-01 -3.72745723e-01  2.63168812e-01 -1.87262148e-01]\n"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","import numpy as np\n","\n","class WordEmbeddingModel:\n","    def __init__(self):\n","        self.model = Word2Vec.load(\"path/to/word2vec.model\")\n","\n","    def encode_text(self, text):\n","        tokens = text.lower().split()\n","        vectors = [self.model.wv[token] for token in tokens if token in self.model.wv]\n","        if vectors:\n","            return np.mean(vectors, axis=0)\n","        else:\n","            return np.zeros(self.model.vector_size)\n","\n","# Usage example\n","if __name__ == \"__main__\":\n","    word2vec_model = WordEmbeddingModel()\n","    text = \"The capital of France is Paris.\"\n","    encoded_text = word2vec_model.encode_text(text)\n","    print(encoded_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"Tp7esa-47_Gy","executionInfo":{"status":"error","timestamp":1717163786360,"user_tz":-210,"elapsed":1302,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"71fe80b3-84eb-4d93-aead-1eb7f5b81573"},"execution_count":116,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'path/to/word2vec.model'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-116-1682457461de>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Usage example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mword2vec_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordEmbeddingModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The capital of France is Paris.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-116-1682457461de>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWordEmbeddingModel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/word2vec.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \"\"\"\n\u001b[0;32m-> 1460\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     fobj = _shortcut_open(\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/word2vec.model'"]}]},{"cell_type":"markdown","source":["# RAG"],"metadata":{"id":"c-HzDeeB69UN"}},{"cell_type":"code","source":["class RAG:\n","  def __init__(self):\n","    pass\n","\n"],"metadata":{"id":"Dws6-d7g-4Ga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -q chromadb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KD2kLBiTEfK","executionInfo":{"status":"ok","timestamp":1717059982198,"user_tz":-210,"elapsed":5996,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"a1af7337-bd76-4a25-9211-bd8f478e3929"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Proceed (Y/n)? y\n"]}]},{"cell_type":"code","source":["!pip install -q chromadb"],"metadata":{"id":"eA0eZOdiwAPq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q chromadb sentence-transformers"],"metadata":{"id":"kWYndIc8QAfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RBHr72hXUXXU","executionInfo":{"status":"ok","timestamp":1717019250880,"user_tz":-210,"elapsed":56205,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"34568ba1-a329-4854-928b-a88fbfc40c5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Using cached sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Installing collected packages: nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n","Successfully installed nvidia-cudnn-cu12-8.9.2.26 nvidia-cusolver-cu12-11.4.5.107 sentence-transformers-3.0.0\n"]}]},{"cell_type":"code","source":["from chromadb import Client, Settings\n","from sentence_transformers import SentenceTransformer\n","\n","# Initialize ChromaDB client\n","client = Client(Settings())\n","\n","# Define a collection for storing contexts\n","collection = client.get_or_create_collection(\"qa_contexts\")\n","\n","# Initialize the sentence transformer model for vectorization\n","vectorizer = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Sample context data\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\"\n","]\n","\n","# Vectorize the context data\n","vectors = vectorizer.encode(context_data, convert_to_tensor=True)\n","\n","# Generate unique IDs for each context\n","ids = [f\"context_{i}\" for i in range(len(context_data))]\n","\n","# Add vectors and context data to ChromaDB\n","collection.add(ids=ids, embeddings=vectors.tolist(), documents=context_data)\n","\n","# Now let's define functions to retrieve context and answer questions\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import torch\n","\n","# Load the model and tokenizer\n","model_name = \"t5-small\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","# Check if GPU is available\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","def retrieve_context(question):\n","    # Vectorize the question\n","    question_vector = vectorizer.encode([question], convert_to_tensor=True)[0]\n","    # Find the most similar context in the collection\n","    results = collection.query(query_embeddings=[question_vector.tolist()], n_results=1)\n","    return results['documents'][0]\n","\n","def answer_question_t5(question):\n","    context = retrieve_context(question)\n","    input_text = f\"question: {question} context: {context}\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","\n","    # Generate answer\n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150)\n","\n","    # Decode the generated text\n","    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return answer.strip()\n","\n","# Example usage\n","question = \"What is the capital of France?\"\n","print(\"Generated Answer:\", answer_question_t5(question))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQJF8IiFP44w","executionInfo":{"status":"ok","timestamp":1717019644854,"user_tz":-210,"elapsed":3353,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"770e40c0-7249-4e49-bf5a-2ae8709dcf79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_0\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_1\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_2\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_0\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_1\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_2\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Answer: Paris\n"]}]},{"cell_type":"code","source":["from chromadb import Client, Settings\n","from sentence_transformers import SentenceTransformer\n","\n","# Initialize ChromaDB client\n","client = Client(Settings())\n","\n","# Define a collection for storing contexts\n","collection = client.get_or_create_collection(\"qa_contexts\")\n","\n","# Initialize the sentence transformer model for vectorization\n","vectorizer = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Sample context data\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South asia.\"\n","]\n","\n","# Vectorize the context data\n","vectors = vectorizer.encode(context_data)\n","\n","# Generate unique IDs for each context\n","ids = [f\"context_{i}\" for i in range(len(context_data))]\n","\n","# Add vectors and context data to ChromaDB\n","collection.add(ids=ids, embeddings=vectors.tolist(), documents=context_data)\n","\n","print(\"Documents added to ChromaDB.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1E-Wu7YtmAI","executionInfo":{"status":"ok","timestamp":1717065128139,"user_tz":-210,"elapsed":561,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"a4069561-b04e-4c60-f628-895226a8abe5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_0\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_1\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Add of existing embedding ID: context_2\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_0\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_1\n","WARNING:chromadb.segment.impl.metadata.sqlite:Insert of existing embedding ID: context_2\n"]},{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n"]}]},{"cell_type":"code","source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import torch\n","\n","# Load the model and tokenizer\n","model_name = \"t5-small\"\n","tokenizer = T5Tokenizer.from_pretrained(model_name)\n","model = T5ForConditionalGeneration.from_pretrained(model_name)\n","\n","# Check if GPU is available\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model.to(device)\n","\n","def retrieve_context(question):\n","    # Vectorize the question\n","    question_vector = vectorizer.encode([question])[0].tolist()\n","    # Find the most similar context in the collection\n","    results = collection.query(query_embeddings=[question_vector], n_results=1)\n","    return results['documents'][0]\n","\n","def answer_question_t5(question):\n","    context = retrieve_context(question)\n","    input_text = f\"question: {question} context: {context}\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","\n","    # Generate answer\n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150)\n","\n","    # Decode the generated text\n","    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return answer.strip()\n","\n","# Example usage\n","question = \"the\"\n","print(\"Generated Answer:\", answer_question_t5(question))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKA1IfExtpaU","executionInfo":{"status":"ok","timestamp":1717065178652,"user_tz":-210,"elapsed":4580,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"8fc4b333-db20-480a-c38d-3d16d5ed99a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["{'ids': [['context_3']], 'distances': [[1.8912122249603271]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South asia.']], 'uris': None, 'data': None}\n","['The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South asia.']\n","Generated Answer: ['The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South asia\n"]}]},{"cell_type":"code","source":["def rag_system(question, top_n=1):\n","    context = retrieve_context(question)\n","    input_text = f\"question: {question} context: {context}\"\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","\n","    # Generate answer\n","    with torch.no_grad():\n","        output = model.generate(input_ids, max_length=150)\n","\n","    # Decode the generated text\n","    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return answer.strip()\n","\n","# Example usage\n","question = \"What is the capital of France?\"\n","response = rag_system(question)\n","print(\"Generated Answer:\", response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FbMiiUyaz_EN","executionInfo":{"status":"ok","timestamp":1717065003619,"user_tz":-210,"elapsed":467,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"890c9175-76f0-4e6c-a985-1c5d5b0119de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'ids': [['context_0']], 'distances': [[0.44983717799186707]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['The capital of France is Paris. It is known for its art, culture, and cuisine.']], 'uris': None, 'data': None}\n","Generated Answer: Paris\n"]}]},{"cell_type":"code","source":["# Example usage\n","question = \"the\"\n","response = rag_system(question)\n","print(\"Generated Answer:\", response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XK9L8HK5OP5","executionInfo":{"status":"ok","timestamp":1717065040760,"user_tz":-210,"elapsed":1184,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"e4b8ebc2-6a18-4b08-ff4a-28ac80609c0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'ids': [['context_2']], 'distances': [[1.9055213928222656]], 'metadatas': [[None]], 'embeddings': None, 'documents': [['The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.']], 'uris': None, 'data': None}\n","Generated Answer: ['The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lZ9Xway0-Z4g"},"execution_count":null,"outputs":[]}]}