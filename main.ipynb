{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1xGeaM-oFVeEliD38KtLRTtP4lPrQVkz-","authorship_tag":"ABX9TyPX1ifpsIdTwW4qSHi9HDdF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"XyuFSQGWlMrD"}},{"cell_type":"code","source":["!pip install -q chromadb sentence-transformers pymupdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xxPdB5Lm6zpx","executionInfo":{"status":"ok","timestamp":1717779494598,"user_tz":-210,"elapsed":136442,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"5f4c4a9f-fe76-4349-c8c4-b8488bcd8a88"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install -q chromadb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T8AWjbKsIDL4","executionInfo":{"status":"ok","timestamp":1717669405776,"user_tz":-210,"elapsed":36632,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"4bfe6c54-a4a6-42a2-a667-5850509509ba","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n","weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install -q langchain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bS6oQJeVSRTH","executionInfo":{"status":"ok","timestamp":1717609462111,"user_tz":-210,"elapsed":9225,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"43a7c35d-195a-4b0b-df15-076807ba03ee","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n","Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.71)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","import logging\n","from transformers import (\n","    GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration,\n","    BertTokenizer, BertForQuestionAnswering, DistilBertTokenizer, DistilBertForQuestionAnswering,\n","    GPTNeoForCausalLM, pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, AutoModelForCausalLM\n",")\n","import torch\n","from chromadb import Client, Settings\n","from sentence_transformers import SentenceTransformer\n","import gc\n","import fitz"],"metadata":{"id":"mJQSWTpCxjb2","executionInfo":{"status":"ok","timestamp":1717779517376,"user_tz":-210,"elapsed":22782,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/Colab Notebooks/RAG'"],"metadata":{"id":"5wT2fvm20bmm","executionInfo":{"status":"ok","timestamp":1717779517377,"user_tz":-210,"elapsed":7,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# GPU"],"metadata":{"id":"-51Pl889lKDn"}},{"cell_type":"code","source":["\n","print(torch.cuda.is_available())\n","# print(torch.cuda.get_device_name(0))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zd5YQ-IfoDFN","executionInfo":{"status":"ok","timestamp":1717779517378,"user_tz":-210,"elapsed":8,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"e3875d19-fa57-4b0c-890b-3c2959ce1360"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]}]},{"cell_type":"markdown","source":["# Hugging Face Login"],"metadata":{"id":"dPnOa3oLlRy2"}},{"cell_type":"code","source":["# Log in using your Hugging Face access token\n","access_token = \"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\"\n","login(access_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QvoS3g0665l","executionInfo":{"status":"ok","timestamp":1717779517379,"user_tz":-210,"elapsed":8,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"e62f8e50-ecee-4708-a919-9d1a3455e4ee"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: read).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}]},{"cell_type":"markdown","source":["# LLM\n","\n","\n","---\n","\n","###key functions of LLM class:\n","\n","\n","*   **load_llm_local**: tries to load an llm from google drvie\n","*   **load_llm_online**: loads the llm from hugging face\n","*   **select_device**: if gpu is available it will select it\n","*   **generate_text**: it can generate text based on given prompt"],"metadata":{"id":"_com4NubP5Jv"}},{"cell_type":"code","source":["class LLM:\n","    model_classes = {\n","        'gpt2': (GPT2Tokenizer, GPT2LMHeadModel, 'gpt2'),\n","        't5': (T5Tokenizer, T5ForConditionalGeneration, 't5-small'),\n","        'bert': (BertTokenizer, BertForQuestionAnswering, 'bert-large-uncased-whole-word-masking-finetuned-squad'),\n","        'distil-bert': (DistilBertTokenizer, DistilBertForQuestionAnswering, 'distilbert-base-cased-distilled-squad'),\n","        'gpt-neo': (GPT2Tokenizer, GPTNeoForCausalLM, 'EleutherAI/gpt-neo-1.3B'),\n","        'gemma': (AutoTokenizer, AutoModelForCausalLM, 'google/gemma-2b-it')\n","    }\n","    def __init__(self, llm_type: str, load_online=False, save_model=False):\n","        self.device = self.select_device()\n","        self.tokenizer, self.model = self.load_llm(llm_type, load_online, save_model)\n","        self.model.to(self.device)\n","        logging.basicConfig(level=logging.INFO)\n","        logging.info(f\"Model {llm_type} loaded and moved to {self.device}.\")\n","\n","    def load_llm(self, llm_type: str, load_online: bool, save_model: bool):\n","\n","        tokenizer_class, model_class, model_path = self.model_classes[llm_type]\n","\n","        if not load_online:\n","            model_path = f\"{project_path}/models/{model_path}\"\n","\n","        tokenizer = tokenizer_class.from_pretrained(model_path)\n","        model = model_class.from_pretrained(model_path)\n","\n","        if save_model:\n","            tokenizer.save_pretrained(f'{project_path}/models/{model_path}')\n","            model.save_pretrained(f'{project_path}/models/{model_path}')\n","\n","        return tokenizer, model\n","\n","\n","    @staticmethod\n","    def select_device() -> str:\n","        return 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        raise NotImplementedError(\"The generate_text method should be implemented by the subclass.\")\n","\n","    def free_memory(self):\n","        del self.model\n","        del self.tokenizer\n","        gc.collect()\n","        torch.cuda.empty_cache()"],"metadata":{"id":"DRAdIqF3kqJo","executionInfo":{"status":"ok","timestamp":1717779517379,"user_tz":-210,"elapsed":6,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## GPT2"],"metadata":{"id":"mTRHbbc58yIz"}},{"cell_type":"code","source":["class GPT2(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gpt2', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        prompt = f\"Context: {context}\\nQuestion: {input_text}\\nAnswer:\"\n","        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n","        outputs = self.model.generate(\n","            inputs,\n","            max_length=80,\n","            temperature=0.7,\n","            top_p=0.9,\n","            top_k=50,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            do_sample=True\n","        )\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = response.replace(prompt, '').strip()\n","        return response.split('\\n')[0]"],"metadata":{"id":"54UnFDdY9Hzl","executionInfo":{"status":"ok","timestamp":1717783896463,"user_tz":-210,"elapsed":425,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["## t5"],"metadata":{"id":"bTjBZyNpEezi"}},{"cell_type":"code","source":["class T5(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('t5', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = None) -> str:\n","        prompt = f\"question: {input_text} context: {context}\" if context else f\"question: {input_text}\"\n","        input_ids = self.tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True).to(self.device)\n","        outputs = self.model.generate(input_ids, max_length=50, num_beams=1, early_stopping=False)\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return response"],"metadata":{"id":"CS6-G5GMEeDe","executionInfo":{"status":"ok","timestamp":1717779517379,"user_tz":-210,"elapsed":5,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## BERT"],"metadata":{"id":"kZqUha0OG81c"}},{"cell_type":"code","source":["class BERT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('bert', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        self.model.to(self.device)\n","        nlp = pipeline('question-answering', model=self.model, tokenizer=self.tokenizer, device=0 if self.device == 'cuda' else -1)\n","        result = nlp(question=input_text, context=context)\n","        return result['answer']"],"metadata":{"id":"00dae2leG-LT","executionInfo":{"status":"ok","timestamp":1717779517803,"user_tz":-210,"elapsed":429,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Distil BERT"],"metadata":{"id":"YBkBRlkZJMPx"}},{"cell_type":"code","source":["class DistilBERT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('distil-bert', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        nlp = pipeline('question-answering', model=self.model, tokenizer=self.tokenizer, device=0 if self.device == 'cuda' else -1)\n","        result = nlp(question=input_text, context=context)\n","        return result['answer']"],"metadata":{"id":"muUrhJFlJOUU","executionInfo":{"status":"ok","timestamp":1717779517804,"user_tz":-210,"elapsed":3,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## GPT-Neo"],"metadata":{"id":"Cme1-CrhLlJ2"}},{"cell_type":"code","source":["class NeoGPT(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gpt-neo', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = None) -> str:\n","        prompt = f\"answer this question: {input_text}\\nbased on this context: {context}\" if context else f\"question: {input_text}\"\n","        inputs = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n","        outputs = self.model.generate(\n","            inputs,\n","            max_length=100,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            num_beams=5,\n","            temperature=0.7,\n","            top_k=50,\n","            top_p=0.95,\n","            no_repeat_ngram_size=2,\n","            do_sample=True\n","        )\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        return response"],"metadata":{"id":"OlHq6oDMLnNc","executionInfo":{"status":"ok","timestamp":1717784182961,"user_tz":-210,"elapsed":396,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["## Gemma"],"metadata":{"id":"yJmD43xgSJrm"}},{"cell_type":"code","source":["class Gemma(LLM):\n","    def __init__(self, load_online=False, save_model=False):\n","        super().__init__('gemma', load_online, save_model)\n","\n","    def generate_text(self, input_text: str, context: str = '') -> str:\n","        prompt = f\"Context: {context}\\nQuestion: {input_text}\\nAnswer:\"\n","        inputs = self.tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True)\n","        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n","\n","        outputs = self.model.generate(\n","            **inputs,\n","            max_length=80,\n","            temperature=0.7,\n","            top_p=0.9,\n","            top_k=50,\n","            num_return_sequences=1,\n","            pad_token_id=self.tokenizer.eos_token_id,\n","            do_sample=True\n","        )\n","\n","        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = response.replace(prompt, '').strip()\n","\n","        return response"],"metadata":{"id":"LCgib4NxRuvv","executionInfo":{"status":"ok","timestamp":1717779517804,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## test LLM class"],"metadata":{"id":"Myfojpj6wtID"}},{"cell_type":"code","source":["llm = GPT2()\n","# llm = Gemma(load_online=True)"],"metadata":{"id":"sBG1ITO4-Dbk","executionInfo":{"status":"ok","timestamp":1717783902460,"user_tz":-210,"elapsed":1050,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","# question = \"say my name.\"\n","# context = 'my name is walter white.'\n","question = \"What is the capital of Iran?\"\n","context = 'tehran is the capital of iran.'\n","\n","response = llm.generate_text(question, context)\n","print(\"response:\", response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-yTtMcNRODtk","executionInfo":{"status":"ok","timestamp":1717783994957,"user_tz":-210,"elapsed":5204,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"07825fb3-0da1-46a6-c6b9-01d63b6103db"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["response: Tehran is the capital of iran.\n"]}]},{"cell_type":"markdown","source":["## free up memmory"],"metadata":{"id":"G12Zw0BtMu7o"}},{"cell_type":"code","source":["gc.collect()\n","torch.cuda.empty_cache()\n","llm.free_memory()"],"metadata":{"id":"f2O7gPxqMn86","executionInfo":{"status":"ok","timestamp":1717774331860,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["# Collection"],"metadata":{"id":"LdexZkZ662Dt"}},{"cell_type":"code","source":["class Collection:\n","    def __init__(self, collection_name: str, transformer_type: str = 'all-MiniLM-L6-v2', load_online=False, save_transformer=False):\n","        self.client = Client(Settings())\n","        existing_collections = [col.name for col in self.client.list_collections()]\n","        if collection_name in existing_collections:\n","            self.client.delete_collection(collection_name)\n","        self.collection = self.client.get_or_create_collection(collection_name)\n","        self.vectorizer = self.load_sentence_transformer(transformer_type, load_online, save_transformer)\n","\n","    def load_sentence_transformer(self, transformer_type: str, load_online: bool, save_transformer: bool):\n","        transformer_path = f'{project_path}/models/{transformer_type}' if not load_online else transformer_type\n","        vectorizer = SentenceTransformer(transformer_path)\n","\n","        if save_transformer:\n","            vectorizer.save(f'{project_path}/models/{transformer_type}')\n","\n","        return vectorizer\n","\n","    def add_contexts(self, context_data: list):\n","        vectors = self.vectorizer.encode(context_data)\n","        ids = [f\"context_{i}\" for i in range(len(context_data))]\n","        self.collection.add(ids=ids, embeddings=vectors.tolist(), documents=context_data)\n","        print(\"Documents added to ChromaDB.\")\n","\n","    def retrieve_contexts(self, question: str, top_n: int = 1):\n","        question_vector = self.vectorizer.encode([question])[0].tolist()\n","        results = self.collection.query(query_embeddings=[question_vector], n_results=top_n)\n","        results = results['documents'][0]\n","        return results[:top_n]"],"metadata":{"id":"AQIdviiA6i4H","executionInfo":{"status":"ok","timestamp":1717774130274,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## test Collection class"],"metadata":{"id":"ovlSNOQQwwPb"}},{"cell_type":"code","source":["# tranformer types:\n","\n","# default => all-MiniLM-L6-v2\n","# paraphrase-MiniLM-L6-v2\n","# paraphrase-xlm-r-multilingual-v1\n","# stsb-roberta-large"],"metadata":{"id":"DM5CKh11UQJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["collection = Collection('rag')\n","\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","collection.add_contexts(context_data)"],"metadata":{"id":"FzCnp-Znw47j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = collection.retrieve_contexts('amazon', top_n=2)\n","\n","print(response)"],"metadata":{"id":"OYQlpAekWYzB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# RAG"],"metadata":{"id":"c-HzDeeB69UN"}},{"cell_type":"code","source":["class RAG:\n","    def __init__(self, llm: LLM, collection: Collection):\n","        self.llm = llm\n","        self.collection = collection\n","\n","    def generate_response(self, query: str, top_n: int=1) -> str:\n","        retrieved_contexts = self.collection.retrieve_contexts(query, top_n)\n","        retrieved_contexts = '\\n'.join(retrieved_contexts)\n","        response = self.llm.generate_text(query, retrieved_contexts)\n","        return response\n"],"metadata":{"id":"pRN9IYwTHNAN","executionInfo":{"status":"ok","timestamp":1717774133053,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## test RAG class"],"metadata":{"id":"sNCKr-Kaw0NY"}},{"cell_type":"code","source":["llm = BERT()\n","collection = Collection('rag')\n","\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","collection.add_contexts(context_data)"],"metadata":{"id":"jYTZaTUOHOc8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rag = RAG(llm, collection)"],"metadata":{"id":"2VmxmObZHpeb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query = \"tell me about china?\"\n","response = rag.generate_response(query, top_n=3)\n","print(response)"],"metadata":{"id":"EqIbl7bPbXrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# PDF"],"metadata":{"id":"0ylfgAmLrWjQ"}},{"cell_type":"code","source":["def extract_text_from_pdf(pdf_path):\n","    doc = fitz.open(pdf_path)\n","    text = \"\"\n","    for page_num in range(len(doc)):\n","        page = doc.load_page(page_num)\n","        text += page.get_text(\"text\")\n","    return text\n"],"metadata":{"id":"21pFJVfPrcFH","executionInfo":{"status":"ok","timestamp":1717774136000,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def preprocess_text(text, chunk_size=500):\n","    text = text.replace('\\n', ' ')  # Replace newlines with spaces\n","    sentences = text.split('. ')  # Split into sentences\n","    chunks = []\n","    chunk = []\n","    length = 0\n","\n","    for sentence in sentences:\n","        chunk.append(sentence)\n","        length += len(sentence.split())\n","\n","        if length > chunk_size:\n","            chunks.append(' '.join(chunk))\n","            chunk = []\n","            length = 0\n","\n","    if chunk:\n","        chunks.append(' '.join(chunk))\n","\n","    return chunks\n"],"metadata":{"id":"78axRor7rfev","executionInfo":{"status":"ok","timestamp":1717774137338,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["pdf_path = f\"{project_path}/tests/micro led 1.pdf\"\n","text = extract_text_from_pdf(pdf_path)\n","contexts = preprocess_text(text)"],"metadata":{"id":"4Q6xtFE3rgOI","executionInfo":{"status":"ok","timestamp":1717774139350,"user_tz":-210,"elapsed":1,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["collection = Collection('rag')\n","collection.add_contexts(contexts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXAVjf3mud5I","executionInfo":{"status":"ok","timestamp":1717774150525,"user_tz":-210,"elapsed":10191,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"a0f25912-0999-4c93-8ab2-2dbeeedd8753"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n"]}]},{"cell_type":"code","source":["# llm = NeoGPT(load_online=True)\n","# llm = BERT()\n","llm = GPT2()"],"metadata":{"id":"f5zc3ii4ufPI","executionInfo":{"status":"ok","timestamp":1717774362201,"user_tz":-210,"elapsed":5096,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["rag = RAG(llm, collection)"],"metadata":{"id":"x7TmnoW0uqeJ","executionInfo":{"status":"ok","timestamp":1717774362202,"user_tz":-210,"elapsed":4,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["question = 'what is One of the most widely anticipated applications of microLEDs?'\n","real_context = \"One of the most widely anticipated applications of microLEDs and RGB capability is in next-generation self-emissive display technol-ogy.10,11\"\n","\n","question = 'what we should expect from adoption of microled displays?'\n","real_context = \"The adoption of microLEDs in display devices is expected to enable much brighter, broader color gamut, longer lifetime, and extremely high pixels per inch (PPI) displays.\"\n","\n","response = rag.generate_response(question, 1)\n","\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"Owx0UKj0uwMX","executionInfo":{"status":"error","timestamp":1717774386351,"user_tz":-210,"elapsed":495,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"97965e87-8969-4702-afcf-0085e734f54f"},"execution_count":34,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-99d342fa1f76>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreal_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The adoption of microLEDs in display devices is expected to enable much brighter, broader color gamut, longer lifetime, and extremely high pixels per inch (PPI) displays.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-f39e5a870b2a>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(self, query, top_n)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mretrieved_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mretrieved_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-340077452365>\u001b[0m in \u001b[0;36mretrieve_contexts\u001b[0;34m(self, question, top_n)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mretrieve_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mquestion_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion_vector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    511\u001b[0m                         )\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentence_transformers/util.py\u001b[0m in \u001b[0;36mbatch_to_device\u001b[0;34m(batch, target_device)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# Example usage\n","question = \"say my name.\"\n","context = 'my name is walter white.'\n","# question = \"What is the capital of Iran?\"\n","# context = 'The capital of Iran is Tehran.'\n","\n","response = llm.generate_text(question, context)\n","print(\"response:\", response)"],"metadata":{"id":"MyrPFovJupao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain_community.embeddings.sentence_transformer import (\n","    SentenceTransformerEmbeddings,\n",")\n","import chromadb\n","from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n","ef = SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')\n","\n","chroma_client = chromadb.Client()\n","collection_name = \"marmikpandya\"\n","try:\n","    chroma_client.delete_collection(collection_name)\n","    print(f\"Deleted existing collection: {collection_name}\")\n","except Exception as e:\n","    print(f\"Collection {collection_name} does not exist or could not be deleted: {e}\")\n","\n","# Create the collection\n","collection = chroma_client.create_collection(name=collection_name, embedding_function=ef)\n","# collection = chroma_client.create_collection(name=\"marmikpandya\", embedding_function=ef)\n","context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","collection.add(\n","    documents=context_data,\n","    # metadatas=[{\"response\": out} for out in dataset[\"train\"][\"output\"]],\n","    ids=[str(i) for i in range(len(context_data))]\n",")\n","chroma_client = chromadb.Client()\n","ef_lc = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","langchain_chroma = Chroma(\n","    client=chroma_client,\n","    collection_name=\"marmikpandya\",\n","    embedding_function=ef_lc,\n",")\n","retriever = langchain_chroma.as_retriever()\n","\n","from langchain.chains import create_retrieval_chain\n","\n","from langchain.chains.combine_documents import create_stuff_documents_chain\n","from langchain_core.prompts import ChatPromptTemplate\n","prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n","\n","<context>\n","{context}\n","</context>\n","\n","Question: {input}\"\"\")\n","\n","document_chain = create_stuff_documents_chain(llm, prompt)\n","retrieval_chain = create_retrieval_chain(retriever, document_chain)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ZmIe8Ui5nVBD","executionInfo":{"status":"error","timestamp":1717595427416,"user_tz":-210,"elapsed":6039,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"21dfa233-5974-469c-f755-7aa645f4b923"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted existing collection: marmikpandya\n"]},{"output_type":"error","ename":"TypeError","evalue":"Expected a Runnable, callable or dict.Instead got an unsupported type: <class '__main__.LLM'>","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-8d486f2cda7c>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf_pipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mdocument_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mretrieval_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/stuff.py\u001b[0m in \u001b[0;36mcreate_stuff_documents_chain\u001b[0;34m(llm, prompt, output_parser, document_prompt, document_separator)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     return (\n\u001b[0;32m---> 84\u001b[0;31m         RunnablePassthrough.assign(**{DOCUMENTS_KEY: format_docs}).with_config(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"format_inputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__or__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2348\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2350\u001b[0;31m                 \u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2351\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2352\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   4900\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4901\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4902\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m   4903\u001b[0m             \u001b[0;34mf\"Expected a Runnable, callable or dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4904\u001b[0m             \u001b[0;34mf\"Instead got an unsupported type: {type(thing)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class '__main__.LLM'>"]}]},{"cell_type":"code","source":["context_data = [\n","    \"The capital of France is Paris. It is known for its art, culture, and cuisine.\",\n","    \"The Great Wall of China is one of the greatest wonders of the world.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South America.\",\n","    \"The Amazon rainforest is a moist broadleaf forest that covers most of the Amazon basin of South Asia.\"\n","]\n","\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='all-MiniLM-L6-v2')\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='paraphrase-MiniLM-L6-v2')\n","collection = Collection(collection_name=\"qa_contexts\", model_name='paraphrase-xlm-r-multilingual-v1')\n","# collection = Collection(collection_name=\"qa_contexts\", model_name='stsb-roberta-large')\n","\n","# Add contexts to the collection\n","collection.add_contexts(context_data)\n","\n","# Retrieve a context based on a question\n","question = \"What is the capital of France?\"\n","context = collection.retrieve_contexts(question)\n","print(f\"Retrieved context: {context}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoDt7bZjB_kI","executionInfo":{"status":"ok","timestamp":1717528413495,"user_tz":-210,"elapsed":9035,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"70a1e3be-f2d6-4067-ebd2-3dfe5133663a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Documents added to ChromaDB.\n","Retrieved context: ['The capital of France is Paris. It is known for its art, culture, and cuisine.']\n"]}]},{"cell_type":"code","source":["!pip install -q langchain_chroma langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6C6QNGhoXgcH","executionInfo":{"status":"ok","timestamp":1717590461468,"user_tz":-210,"elapsed":16369,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"9f4b6e44-804c-47e6-d58e-e8806e690dc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import logging\n","from sentence_transformers import SentenceTransformer\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForMaskedLM, DistilBertTokenizer, DistilBertForMaskedLM, RobertaTokenizer, RobertaForMaskedLM, GPTNeoForCausalLM\n","import torch\n","from chromadb import Client, Settings\n","from langchain.chains import create_retrieval_chain\n","# from langchain.llms import OpenAI\n","from langchain_chroma import Chroma\n","from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings"],"metadata":{"id":"zGb10OtRXcF7"},"execution_count":null,"outputs":[]}]}