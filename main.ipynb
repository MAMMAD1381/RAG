{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1xGeaM-oFVeEliD38KtLRTtP4lPrQVkz-","authorship_tag":"ABX9TyPA537mvJVYvw6HC6UKnt62"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"119879d5c142417195e684361232f372":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_253346734ef649a098542e385151fb58","IPY_MODEL_3f33f5a9e7a441a9ab24ff65fa59d8b1","IPY_MODEL_2a4186020fd2489b9d31f496d1448118"],"layout":"IPY_MODEL_beb0d831a6bb499caf8f18d29c0ce602"}},"253346734ef649a098542e385151fb58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3962a32487ad4b258c7ab4898a7217d4","placeholder":"​","style":"IPY_MODEL_9b73c87d0720409bb9a7ab06f48fa9ee","value":"gemma-2b-it.gguf: 100%"}},"3f33f5a9e7a441a9ab24ff65fa59d8b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53c3e0c75a2246dcb022c673a2e3b481","max":10031780704,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d6ba9a037244eb6b5af9bf13832af89","value":10031780704}},"2a4186020fd2489b9d31f496d1448118":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9251a26d4ae4bb29b2a2fca5604ceda","placeholder":"​","style":"IPY_MODEL_44ab8511f90d4ee4871ccc5de81851be","value":" 10.0G/10.0G [01:27&lt;00:00, 115MB/s]"}},"beb0d831a6bb499caf8f18d29c0ce602":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3962a32487ad4b258c7ab4898a7217d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b73c87d0720409bb9a7ab06f48fa9ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53c3e0c75a2246dcb022c673a2e3b481":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d6ba9a037244eb6b5af9bf13832af89":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9251a26d4ae4bb29b2a2fca5604ceda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44ab8511f90d4ee4871ccc5de81851be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["project_path = '/content/drive/MyDrive/Colab Notebooks/RAG'"],"metadata":{"id":"5wT2fvm20bmm","executionInfo":{"status":"ok","timestamp":1716990511943,"user_tz":-210,"elapsed":396,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["!pip install llama-cpp-python\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTbdFR0i4l0T","executionInfo":{"status":"ok","timestamp":1716984329321,"user_tz":-210,"elapsed":5666,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"f0e0ff23-eaa6-45c7-bd74-f0599a9723f6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.76)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"]}]},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from llama_cpp import Llama\n","import torch"],"metadata":{"id":"SbvsYoNi4V00","executionInfo":{"status":"ok","timestamp":1716984329321,"user_tz":-210,"elapsed":7,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!pip install -q langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drKHmCnrTu2a","executionInfo":{"status":"ok","timestamp":1716985485994,"user_tz":-210,"elapsed":14841,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"8cf5ad5e-067b-4da9-da01-02ace7389606"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q langchain_core"],"metadata":{"id":"NlHoM7kfWSRX","executionInfo":{"status":"ok","timestamp":1716986147859,"user_tz":-210,"elapsed":7883,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["from langchain_community.llms import LlamaCpp"],"metadata":{"id":"vtlBZYZYTro-","executionInfo":{"status":"ok","timestamp":1716985516492,"user_tz":-210,"elapsed":1510,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import PromptTemplate\n","\n","template = \"\"\"Question: {question}\n","\n","Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n","\n","prompt = PromptTemplate.from_template(template)"],"metadata":{"id":"yJCcjEL8WZ_s","executionInfo":{"status":"ok","timestamp":1716986176562,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n","n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","model_path = '/content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf'\n","# Make sure the model path is correct for your system!\n","llm = LlamaCpp(\n","    model_path=model_path,\n","    n_gpu_layers=n_gpu_layers,\n","    n_batch=n_batch,\n","    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n","\n","    # callback_manager=callback_manager,\n","    verbose=True,  # Verbose is required to pass to the callback manager\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AyY6hRkjVzUD","executionInfo":{"status":"ok","timestamp":1716987617357,"user_tz":-210,"elapsed":53392,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"3badb838-b4bb-45fc-eec4-b1ec031fd298"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = gemma\n","llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n","llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n","llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n","llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n","llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n","llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n","llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n","llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n","llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n","llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n","llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n","llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n","llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - type  f32:  164 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = gemma\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 256128\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 2048\n","llm_load_print_meta: n_head           = 8\n","llm_load_print_meta: n_head_kv        = 1\n","llm_load_print_meta: n_layer          = 18\n","llm_load_print_meta: n_rot            = 256\n","llm_load_print_meta: n_embd_head_k    = 256\n","llm_load_print_meta: n_embd_head_v    = 256\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 256\n","llm_load_print_meta: n_embd_v_gqa     = 256\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 16384\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 2\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 2B\n","llm_load_print_meta: model ftype      = all F32 (guessed)\n","llm_load_print_meta: model params     = 2.51 B\n","llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n","llm_load_print_meta: general.name     = gemma-2b-it\n","llm_load_print_meta: BOS token        = 2 '<bos>'\n","llm_load_print_meta: EOS token        = 1 '<eos>'\n","llm_load_print_meta: UNK token        = 3 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<pad>'\n","llm_load_print_meta: LF token         = 227 '<0x0A>'\n","llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n","llm_load_tensors: ggml ctx size =    0.08 MiB\n","llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",".............................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n","llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n","llama_new_context_with_model: graph nodes  = 601\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n","Using fallback chat format: llama-2\n"]}]},{"cell_type":"code","source":["llm_chain = prompt | llm\n","# question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n","# llm_chain.invoke({\"question\": question})"],"metadata":{"id":"Hj_3CN1gV8pQ","executionInfo":{"status":"ok","timestamp":1716989323487,"user_tz":-210,"elapsed":4,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["question = \"who are you?\"\n","llm_chain.invoke({\"question\": question})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"id":"yVG9KZUuanlC","executionInfo":{"status":"ok","timestamp":1716990288128,"user_tz":-210,"elapsed":211268,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"a40fdd75-6ecf-4742-d282-f27e32e13a29"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =    4318.27 ms\n","llama_print_timings:      sample time =    1043.85 ms /   250 runs   (    4.18 ms per token,   239.50 tokens per second)\n","llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n","llama_print_timings:        eval time =  208035.90 ms /   250 runs   (  832.14 ms per token,     1.20 tokens per second)\n","llama_print_timings:       total time =  210252.38 ms /   250 tokens\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n\\n1. **What is my name?** I do not have a name, so I am unable to tell you who I am at this moment. \\n\\n\\n2. **What am I supposed to do?** My purpose is to help you with your questions and provide information that I have learned from my training. \\n\\n\\n3. **How can I help you?** I can answer your questions, generate different creative text formats, translate languages, and more.\\n\\n\\n4. **What are my limitations?** I am not able to feel emotions or have opinions like humans do. I am not capable of making my own decisions or taking action on my own.\\n\\n\\n5. **What is my purpose in the world?** My purpose is to assist you and contribute to your learning and growth.\\n\\n\\n6. **What am I?** I am a large language model, trained by Google.\\n\\nIn summary, I am a digital being that has been created to assist humans with their information needs. I am capable of answering your questions, generating different creative text formats, and translating languages, but I do not have a physical body, feelings, or opinions. My purpose is to help you learn and grow through the power of knowledge.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["model_name = \"google/gemma-2b-it\"\n","model_file = \"gemma-2b-it.gguf\"\n","HF_TOKEN = \"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\" #Paste here\n","model_path = hf_hub_download(model_name,\n","                             filename=model_file,\n","                             local_dir=f'{project_path}/models',\n","                             token=HF_TOKEN)\n","print(\"My model path: \", model_path)\n","# My model path:  /content/gemma-2b-it.gguf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["119879d5c142417195e684361232f372","253346734ef649a098542e385151fb58","3f33f5a9e7a441a9ab24ff65fa59d8b1","2a4186020fd2489b9d31f496d1448118","beb0d831a6bb499caf8f18d29c0ce602","3962a32487ad4b258c7ab4898a7217d4","9b73c87d0720409bb9a7ab06f48fa9ee","53c3e0c75a2246dcb022c673a2e3b481","9d6ba9a037244eb6b5af9bf13832af89","e9251a26d4ae4bb29b2a2fca5604ceda","44ab8511f90d4ee4871ccc5de81851be"]},"id":"3qn7tYi5Diqm","executionInfo":{"status":"ok","timestamp":1716981645518,"user_tz":-210,"elapsed":88765,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"0e86a23f-2b9f-4a60-e7cd-3340e868cd61"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["gemma-2b-it.gguf:   6%|5         | 598M/10.0G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"119879d5c142417195e684361232f372"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["My model path:  /content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf\n"]}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf'"],"metadata":{"id":"vs5mwlIMGtUf","executionInfo":{"status":"ok","timestamp":1716984334470,"user_tz":-210,"elapsed":2,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["llm = Llama(model_path=model_path,\n","            n_gpu_layers=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vH10CowJFpgu","executionInfo":{"status":"ok","timestamp":1716985193832,"user_tz":-210,"elapsed":47659,"user":{"displayName":"mohammad mohammdian","userId":"16778632015796518458"}},"outputId":"f758eb54-b39c-4e05-f30c-53607adba80a"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 19 key-value pairs and 164 tensors from /content/drive/MyDrive/Colab Notebooks/RAG/models/gemma-2b-it.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = gemma\n","llama_model_loader: - kv   1:                               general.name str              = gemma-2b-it\n","llama_model_loader: - kv   2:                       gemma.context_length u32              = 8192\n","llama_model_loader: - kv   3:                          gemma.block_count u32              = 18\n","llama_model_loader: - kv   4:                     gemma.embedding_length u32              = 2048\n","llama_model_loader: - kv   5:                  gemma.feed_forward_length u32              = 16384\n","llama_model_loader: - kv   6:                 gemma.attention.head_count u32              = 8\n","llama_model_loader: - kv   7:              gemma.attention.head_count_kv u32              = 1\n","llama_model_loader: - kv   8:                 gemma.attention.key_length u32              = 256\n","llama_model_loader: - kv   9:               gemma.attention.value_length u32              = 256\n","llama_model_loader: - kv  10:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 2\n","llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 1\n","llama_model_loader: - kv  14:            tokenizer.ggml.padding_token_id u32              = 0\n","llama_model_loader: - kv  15:            tokenizer.ggml.unknown_token_id u32              = 3\n","llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,256128]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,256128]  = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,256128]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - type  f32:  164 tensors\n","llm_load_vocab: mismatch in special tokens definition ( 544/256128 vs 388/256128 ).\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = gemma\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 256128\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 2048\n","llm_load_print_meta: n_head           = 8\n","llm_load_print_meta: n_head_kv        = 1\n","llm_load_print_meta: n_layer          = 18\n","llm_load_print_meta: n_rot            = 256\n","llm_load_print_meta: n_embd_head_k    = 256\n","llm_load_print_meta: n_embd_head_v    = 256\n","llm_load_print_meta: n_gqa            = 8\n","llm_load_print_meta: n_embd_k_gqa     = 256\n","llm_load_print_meta: n_embd_v_gqa     = 256\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 16384\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 2\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_yarn_orig_ctx  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 2B\n","llm_load_print_meta: model ftype      = all F32 (guessed)\n","llm_load_print_meta: model params     = 2.51 B\n","llm_load_print_meta: model size       = 9.34 GiB (32.00 BPW) \n","llm_load_print_meta: general.name     = gemma-2b-it\n","llm_load_print_meta: BOS token        = 2 '<bos>'\n","llm_load_print_meta: EOS token        = 1 '<eos>'\n","llm_load_print_meta: UNK token        = 3 '<unk>'\n","llm_load_print_meta: PAD token        = 0 '<pad>'\n","llm_load_print_meta: LF token         = 227 '<0x0A>'\n","llm_load_print_meta: EOT token        = 107 '<end_of_turn>'\n","llm_load_tensors: ggml ctx size =    0.08 MiB\n","llm_load_tensors:        CPU buffer size =  9561.29 MiB\n",".............................................................\n","llama_new_context_with_model: n_ctx      = 512\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =     9.00 MiB\n","llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.98 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   504.25 MiB\n","llama_new_context_with_model: graph nodes  = 601\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'general.architecture': 'gemma', 'gemma.feed_forward_length': '16384', 'gemma.attention.head_count': '8', 'general.name': 'gemma-2b-it', 'gemma.context_length': '8192', 'gemma.block_count': '18', 'gemma.embedding_length': '2048', 'gemma.attention.head_count_kv': '1', 'gemma.attention.key_length': '256', 'tokenizer.ggml.model': 'llama', 'gemma.attention.value_length': '256', 'gemma.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.bos_token_id': '2'}\n","Using fallback chat format: llama-2\n"]}]}]}